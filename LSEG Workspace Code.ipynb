{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b190f93-21ec-474b-a2aa-b24346f2fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import refinitiv.data as rd\n",
    "from refinitiv.data.content import historical_pricing as hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08652a4e-3e1a-4e1c-977c-7ba8da2a4ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Start Refinitiv Session\\nrd.open_session(app_key=\"2ad05903f7cb4f84b75e13735fb71abde98f7d85\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Start Refinitiv Session\n",
    "rd.open_session(app_key=\"2ad05903f7cb4f84b75e13735fb71abde98f7d85\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed41be1-4082-4de9-afbb-03e3cdbd6e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Define instruments and Date range\\nSTART_DATE = \"2006-01-01\"\\nEND_DATE = \"2025-06-30\"\\n\\n# Replace these placeholders with the actual Refinitiv RICs \\nSPX_PROXY_RIC     = \"SPY.P\"   # S&P 500 ETF (proxy for .SPX)\\nDOLLAR_INDEX_RIC  = \".DXY\"    # Dollar Index\\nWTI_OIL_RIC       = \"CLc1\"    # WTI Crude Oil Futures (continuous contract)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Define instruments and Date range\n",
    "START_DATE = \"2006-01-01\"\n",
    "END_DATE = \"2025-06-30\"\n",
    "\n",
    "# Replace these placeholders with the actual Refinitiv RICs \n",
    "SPX_PROXY_RIC     = \"SPY.P\"   # S&P 500 ETF (proxy for .SPX)\n",
    "DOLLAR_INDEX_RIC  = \".DXY\"    # Dollar Index\n",
    "WTI_OIL_RIC       = \"CLc1\"    # WTI Crude Oil Futures (continuous contract)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a750a5-2162-4556-a6b7-fd0b3cfe68c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Obtain required data\\n# --- 1. S&P 500 OHLCV ---\\ndf_spx_raw = hp.summaries.Definition(\\n    universe=SPX_PROXY_RIC,\\n    fields=[\"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"VOLUME\"],\\n    interval=hp.Intervals.DAILY,\\n    start=START_DATE,\\n    end=END_DATE\\n).get_data()\\n\\n# --- 2. Dollar Index & WTI Oil (Closing Prices) ---\\ndf_macro_raw = hp.summaries.Definition(\\n    universe=[DOLLAR_INDEX_RIC, WTI_OIL_RIC],\\n    fields=[\"CLOSE\"],\\n    interval=hp.Intervals.DAILY,\\n    start=START_DATE,\\n    end=END_DATE\\n).get_data()\\n\\n# Output checks\\nprint(\"S&P 500 Raw DataFrame (df_spx_raw):\")\\nprint(df_spx_raw.head())\\n\\nprint(\"\\nMacro Raw DataFrame (df_macro_raw):\")\\nprint(df_macro_raw.head())\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import refinitiv.data as rd\n",
    "#print(\"Refinitiv Data Library version:\", rd.__version__)\n",
    "\n",
    "#print(dir(hp))\n",
    "\n",
    "\"\"\"\n",
    "from refinitiv.data.content import pricing\n",
    "\n",
    "# request latest snapshot for Apple\n",
    "snap = pricing.Definition(universe=\"AAPL.O\").get_data()\n",
    "print(snap)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#Obtain required data\n",
    "# --- 1. S&P 500 OHLCV ---\n",
    "df_spx_raw = hp.summaries.Definition(\n",
    "    universe=SPX_PROXY_RIC,\n",
    "    fields=[\"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"VOLUME\"],\n",
    "    interval=hp.Intervals.DAILY,\n",
    "    start=START_DATE,\n",
    "    end=END_DATE\n",
    ").get_data()\n",
    "\n",
    "# --- 2. Dollar Index & WTI Oil (Closing Prices) ---\n",
    "df_macro_raw = hp.summaries.Definition(\n",
    "    universe=[DOLLAR_INDEX_RIC, WTI_OIL_RIC],\n",
    "    fields=[\"CLOSE\"],\n",
    "    interval=hp.Intervals.DAILY,\n",
    "    start=START_DATE,\n",
    "    end=END_DATE\n",
    ").get_data()\n",
    "\n",
    "# Output checks\n",
    "print(\"S&P 500 Raw DataFrame (df_spx_raw):\")\n",
    "print(df_spx_raw.head())\n",
    "\n",
    "print(\"\\nMacro Raw DataFrame (df_macro_raw):\")\n",
    "print(df_macro_raw.head())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5015018-69ab-4065-acfc-9419418ea601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stationarity checks\n",
    "#SPY log returns\n",
    "adf_result = adfuller(df['SPY_log_ret'])\n",
    "print(f\"ADF Statistic: {adf_result[0]:.3f}, p-value: {adf_result[1]:.5f}\")\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"âœ… SPY_log_ret is stationary.\")\n",
    "else:\n",
    "    print(\"âš ï¸ SPY_log_ret may not be stationary; differencing might be needed.\")\n",
    "\n",
    "\n",
    "#SPY log returns\n",
    "adf_result_1 = adfuller(df['DXY_ret'])\n",
    "print(f\"ADF Statistic: {adf_result[0]:.3f}, p-value: {adf_result[1]:.5f}\")\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"âœ… DXY_ret is stationary.\")\n",
    "else:\n",
    "    print(\"âš ï¸ DXY_ret may not be stationary; differencing might be needed.\")\n",
    "\n",
    "\n",
    "#SPY log returns\n",
    "adf_result_2 = adfuller(df['WTI_ret'])\n",
    "print(f\"ADF Statistic: {adf_result[0]:.3f}, p-value: {adf_result[1]:.5f}\")\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"âœ… WTI_ret is stationary.\")\n",
    "else:\n",
    "    print(\"âš ï¸ WTI_ret may not be stationary; differencing might be needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e973d90-217f-43e3-9023-5855ef2f7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale numerical features\n",
    "features_to_scale = ['SPY_log_ret', 'SPY_p_change', 'DXY_ret', 'WTI_ret']\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[features_to_scale] = scaler.fit_transform(df_scaled[features_to_scale])\n",
    "\n",
    "#Check output\n",
    "print(\"\\nScaled feature sample:\")\n",
    "print(df_scaled[features_to_scale].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0d907-1fd4-4520-8c39-16ef9801c1fa",
   "metadata": {},
   "source": [
    "# Adaptive Wavelet Transform (AWT) Denoising & Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a6686-75d8-4898-9afc-d1d5071d8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries \n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#1. Extract target signal\n",
    "signal = df_scaled['SPY_log_ret'].values  # denoised target variable\n",
    "\n",
    "#2. Choose wavelet parameters\n",
    "wavelet = 'db4'         # Daubechies 4 is widely used in financial time series\n",
    "level = 2               # decomposition level (2â€“4 typical for daily data)\n",
    "\n",
    "#3. Perform discrete wavelet decomposition\n",
    "coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "\n",
    "# where coeffs[0] = Approximation (trend), and\n",
    "# coeffs[1:] = Detail coefficients (short-term fluctuations)\n",
    "\n",
    "#4. Reconstruct the approximation and detail signals\n",
    "approximation = pywt.waverec([coeffs[0]] + [None]*level, wavelet)\n",
    "detail = signal - approximation[:len(signal)]  # ensure same length\n",
    "\n",
    "# Clip both to the same index as df_scaled\n",
    "approximation = approximation[:len(df_scaled)]\n",
    "detail = detail[:len(df_scaled)]\n",
    "\n",
    "#5. Add components to your DataFrame\n",
    "df_scaled['SPY_s'] = approximation\n",
    "df_scaled['SPY_d'] = detail\n",
    "\n",
    "#6. Visual check of output\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df_scaled.index, df_scaled['SPY_log_ret'], label='Original', alpha=0.6)\n",
    "plt.plot(df_scaled.index, df_scaled['SPY_s'], label='Approximation (Trend)', color='orange')\n",
    "plt.plot(df_scaled.index, df_scaled['SPY_d'], label='Detail (Fluctuation)', color='green', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"AWT Decomposition of SPY_log_ret using Daubechies-4\")\n",
    "plt.show()\n",
    "\n",
    "#7. Save the decomposed dataset\n",
    "df_scaled.to_csv(\"03_awt_decomposed_data.csv\")\n",
    "print(\"\\nâœ… AWT decomposition complete. Data saved as '03_awt_decomposed_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92349090-cabf-4792-b66e-3190162a97de",
   "metadata": {},
   "source": [
    "# Feature Engineering: 21 Financial Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e189428-7560-420b-b782-184c05f0addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# DATA PREPARATION & MERGE (for yfinance-style Data)\n",
    "# ================================================================\n",
    "\n",
    "#Import required libraries\n",
    "import re\n",
    "import ta  # technical analysis library\n",
    "\n",
    "# --- 1ï¸âƒ£ Flatten MultiIndex columns if any ---\n",
    "def flatten_columns(df):\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = ['_'.join([str(c) for c in col if c]) for col in df.columns]\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "df_spx_full = flatten_columns(df_spx_raw)\n",
    "df_scaled = flatten_columns(df_scaled)\n",
    "\n",
    "# --- 2ï¸âƒ£ Identify & rename likely OHLCV columns ---\n",
    "ohlcv_candidates = [c for c in df_spx_full.columns if any(x in c.lower() for x in ['open', 'high', 'low', 'close', 'volume'])]\n",
    "rename_map = {}\n",
    "for col in ohlcv_candidates:\n",
    "    lc = col.lower()\n",
    "    if 'open' in lc: rename_map[col] = 'SPY_Open'\n",
    "    elif 'high' in lc: rename_map[col] = 'SPY_High'\n",
    "    elif 'low' in lc: rename_map[col] = 'SPY_Low'\n",
    "    elif 'close' in lc and 'adj' not in lc: rename_map[col] = 'SPY_Close'\n",
    "    elif 'adj' in lc and 'close' in lc: rename_map[col] = 'SPY_Adj_Close'\n",
    "    elif 'volume' in lc: rename_map[col] = 'SPY_Volume'\n",
    "\n",
    "df_spx_full = df_spx_full.rename(columns=rename_map)\n",
    "\n",
    "# --- 3ï¸âƒ£ Drop duplicates and merge safely ---\n",
    "df_scaled = df_scaled.drop(columns=[c for c in ['SPY_Close'] if c in df_scaled.columns], errors='ignore')\n",
    "df_feat = df_scaled.join(\n",
    "    df_spx_full[[c for c in ['SPY_Open', 'SPY_High', 'SPY_Low', 'SPY_Close', 'SPY_Volume'] if c in df_spx_full.columns]],\n",
    "    how='inner',\n",
    "    rsuffix='_orig'\n",
    ")\n",
    "\n",
    "# --- 4ï¸âƒ£ Clean up any whitespace or weird naming ---\n",
    "df_feat.columns = df_feat.columns.map(lambda x: re.sub(r'\\s+', '', str(x)))\n",
    "\n",
    "# --- 5ï¸âƒ£ Confirm required columns exist ---\n",
    "required_cols = ['SPY_Open', 'SPY_High', 'SPY_Low', 'SPY_Close', 'SPY_Volume']\n",
    "missing = [c for c in required_cols if c not in df_feat.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing SPY OHLCV columns: {missing}\")\n",
    "else:\n",
    "    print(\"âœ… All SPY OHLCV columns present.\")\n",
    "\n",
    "print(f\"Final df_feat shape before feature generation: {df_feat.shape}\")\n",
    "print(df_feat.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61a49e-c7e2-472b-8269-bed09d81214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FEATURE ENGINEERING: 21 TECHNICAL INDICATORS (CLEAN VERSION)\n",
    "# ================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta  # technical analysis library\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 0ï¸âƒ£ Ensure clean, unique index\n",
    "# ------------------------------------------------\n",
    "df_feat = df_feat.copy()\n",
    "df_feat = df_feat[~df_feat.index.duplicated(keep='first')]\n",
    "df_feat = df_feat.sort_index()\n",
    "print(f\"âœ… Index cleaned. Unique rows: {len(df_feat)}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1ï¸âƒ£ Confirm required columns\n",
    "# ------------------------------------------------\n",
    "required_cols = ['SPY_Open', 'SPY_High', 'SPY_Low', 'SPY_Close', 'SPY_Volume', 'SPY_s']\n",
    "for c in required_cols:\n",
    "    if c not in df_feat.columns:\n",
    "        raise KeyError(f\"Missing required column: {c}\")\n",
    "\n",
    "print(\"âœ… All required columns confirmed present.\\n\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2ï¸âƒ£ Moving Averages (Trend)\n",
    "# ------------------------------------------------\n",
    "price_col = 'SPY_s'\n",
    "vol_col = 'SPY_Volume'\n",
    "\n",
    "df_feat['EMA20'] = ta.trend.EMAIndicator(df_feat[price_col], window=20).ema_indicator()\n",
    "df_feat['MA5'] = ta.trend.SMAIndicator(df_feat[price_col], window=5).sma_indicator()\n",
    "df_feat['MA10'] = ta.trend.SMAIndicator(df_feat[price_col], window=10).sma_indicator()\n",
    "df_feat['V_MA5'] = ta.trend.SMAIndicator(df_feat[vol_col], window=5).sma_indicator()\n",
    "df_feat['V_MA10'] = ta.trend.SMAIndicator(df_feat[vol_col], window=10).sma_indicator()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3ï¸âƒ£ Momentum Indicators\n",
    "# ------------------------------------------------\n",
    "df_feat['MACD'] = ta.trend.MACD(df_feat[price_col]).macd()\n",
    "df_feat['ROC'] = ta.momentum.ROCIndicator(df_feat[price_col], window=10).roc()\n",
    "df_feat['MTM6'] = df_feat[price_col] / df_feat[price_col].shift(126) - 1\n",
    "df_feat['MTM12'] = df_feat[price_col] / df_feat[price_col].shift(252) - 1\n",
    "\n",
    "stoch = ta.momentum.StochasticOscillator(\n",
    "    high=df_feat['SPY_High'],\n",
    "    low=df_feat['SPY_Low'],\n",
    "    close=df_feat[price_col],\n",
    "    window=14,\n",
    "    smooth_window=3\n",
    ")\n",
    "df_feat['SMI'] = stoch.stoch_signal()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4ï¸âƒ£ Volatility & Pattern Indicators\n",
    "# ------------------------------------------------\n",
    "df_feat['ATR'] = ta.volatility.AverageTrueRange(\n",
    "    high=df_feat['SPY_High'],\n",
    "    low=df_feat['SPY_Low'],\n",
    "    close=df_feat[price_col],\n",
    "    window=14\n",
    ").average_true_range()\n",
    "\n",
    "boll = ta.volatility.BollingerBands(df_feat[price_col], window=20, window_dev=2)\n",
    "df_feat['BOLL_upper'] = boll.bollinger_hband()\n",
    "df_feat['BOLL_lower'] = boll.bollinger_lband()\n",
    "df_feat['BOLL_bandwidth'] = df_feat['BOLL_upper'] - df_feat['BOLL_lower']\n",
    "\n",
    "df_feat['CCI'] = ta.trend.CCIIndicator(\n",
    "    high=df_feat['SPY_High'],\n",
    "    low=df_feat['SPY_Low'],\n",
    "    close=df_feat[price_col],\n",
    "    window=20\n",
    ").cci()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5ï¸âƒ£ Volume-based Indicator (WVAD)\n",
    "# ------------------------------------------------\n",
    "# Ensure alignment and valid numeric dtype\n",
    "for c in ['SPY_Open', 'SPY_High', 'SPY_Low', 'SPY_Close', 'SPY_Volume']:\n",
    "    df_feat[c] = pd.to_numeric(df_feat[c], errors='coerce')\n",
    "\n",
    "df_feat['WVAD'] = (\n",
    "    (df_feat['SPY_Close'] - df_feat['SPY_Open']) /\n",
    "    (df_feat['SPY_High'] - df_feat['SPY_Low'] + 1e-10)\n",
    ") * df_feat['SPY_Volume']\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6ï¸âƒ£ Final Cleanup\n",
    "# ------------------------------------------------\n",
    "df_feat = df_feat.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "print(f\"âœ… Technical indicators successfully generated! Final shape: {df_feat.shape}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 7ï¸âƒ£ Save to file\n",
    "# ------------------------------------------------\n",
    "df_feat.to_csv(\"04_feature_engineered_data.csv\")\n",
    "print(\"ðŸ’¾ Saved as '04_feature_engineered_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1d7ca-63fd-45ec-a755-99e02688e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ï¸âƒ£ OPTIONAL VISUAL CHECKS\n",
    "# ================================================================\n",
    "\n",
    "# --- Plot MACD ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_feat.index, df_feat[price_col], label='SPY_s (Smoothed Price)', color='black', alpha=0.7)\n",
    "plt.plot(df_feat.index, df_feat['EMA20'], label='EMA20', linestyle='--')\n",
    "plt.title(\"Price with EMA20\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_feat.index, df_feat['MACD'], label='MACD', color='blue')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.title(\"MACD Indicator\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Bollinger Bands ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_feat.index, df_feat[price_col], label='SPY_s (Smoothed Price)', color='black')\n",
    "plt.plot(df_feat.index, df_feat['BOLL_upper'], label='Bollinger Upper', linestyle='--', color='red')\n",
    "plt.plot(df_feat.index, df_feat['BOLL_lower'], label='Bollinger Lower', linestyle='--', color='green')\n",
    "plt.fill_between(df_feat.index, df_feat['BOLL_lower'], df_feat['BOLL_upper'], color='gray', alpha=0.1)\n",
    "plt.title(\"Bollinger Bands\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Volume & WVAD ---\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(df_feat.index, df_feat['SPY_Volume'], label='SPY Volume', color='orange', alpha=0.6)\n",
    "plt.plot(df_feat.index, df_feat['WVAD'], label='WVAD', color='purple', alpha=0.8)\n",
    "plt.title(\"Volume and WVAD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533a937-50dc-4585-bfb5-ade66b6d3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# TWO-PANEL VERIFICATION PLOT: Price + Bollinger (top), MACD (bottom)\n",
    "# ================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "# --- Safety: ensure needed columns are present ---\n",
    "_need = ['SPY_s', 'BOLL_upper', 'BOLL_lower', 'MACD']\n",
    "_missing = [c for c in _need if c not in df_feat.columns]\n",
    "if _missing:\n",
    "    raise KeyError(f\"Missing columns for plot: {_missing}\")\n",
    "\n",
    "# Optional: focus window (e.g., last 2 years). Comment out to use full history.\n",
    "# dfp = df_feat.last('730D')  # last ~730 days\n",
    "dfp = df_feat.copy()\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "gs = fig.add_gridspec(nrows=2, ncols=1, height_ratios=[3, 1.4], hspace=0.08)\n",
    "\n",
    "# -------------------------\n",
    "# Top panel: Price + Bands\n",
    "# -------------------------\n",
    "ax_price = fig.add_subplot(gs[0, 0])\n",
    "ax_price.plot(dfp.index, dfp['SPY_s'], label='SPY_s (Smoothed Price)')\n",
    "ax_price.plot(dfp.index, dfp['BOLL_upper'], label='Bollinger Upper', linestyle='--')\n",
    "ax_price.plot(dfp.index, dfp['BOLL_lower'], label='Bollinger Lower', linestyle='--')\n",
    "ax_price.fill_between(dfp.index, dfp['BOLL_lower'], dfp['BOLL_upper'], alpha=0.10)\n",
    "ax_price.set_title(\"SPY_s with Bollinger Bands\")\n",
    "ax_price.legend(loc='upper left')\n",
    "ax_price.grid(True, alpha=0.3)\n",
    "\n",
    "# -------------------------\n",
    "# Bottom panel: MACD\n",
    "# -------------------------\n",
    "ax_macd = fig.add_subplot(gs[1, 0], sharex=ax_price)\n",
    "ax_macd.plot(dfp.index, dfp['MACD'], label='MACD')\n",
    "ax_macd.axhline(0.0, linestyle='--', linewidth=1)\n",
    "ax_macd.set_title(\"MACD\")\n",
    "ax_macd.legend(loc='upper left')\n",
    "ax_macd.grid(True, alpha=0.3)\n",
    "\n",
    "# Tidy x-axis formatting\n",
    "date_fmt = DateFormatter(\"%Y-%m\")\n",
    "ax_macd.xaxis.set_major_formatter(date_fmt)\n",
    "plt.setp(ax_price.get_xticklabels(), visible=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d22f98-2deb-415b-8176-d787011af451",
   "metadata": {},
   "source": [
    "# Fit FIEGARCH(1,d,1)-t in R from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfbec6-70c5-48d0-afc9-8c2080b25b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) rpy2 + R sanity check (Windows-safe ABI mode) ===\n",
    "# - Confirms rpy2 can see your R\n",
    "# - Prints R version and home\n",
    "# - (No pandas2ri.activate used â€” we rely on localconverter only)\n",
    "\n",
    "import os\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "# Optional: reduce thread contention before R loads (helps stability on Windows/Anaconda)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "print(\"R version:\", ro.r(\"R.version.string\")[0])\n",
    "print(\"R home   :\", ro.r(\"R.home()\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350327b-b04c-40f8-a2df-1befcaf7daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as ro\n",
    "\n",
    "r = ro.r\n",
    "\n",
    "# 1) Choose a writable user library and prepend it to .libPaths()\n",
    "r('''\n",
    "# Compute a per-user library path if R didn't define one\n",
    "userlib <- Sys.getenv(\"R_LIBS_USER\")\n",
    "if (is.na(userlib) || userlib == \"\") {\n",
    "  userlib <- file.path(Sys.getenv(\"USERPROFILE\"), \"R\",\n",
    "                       paste0(\"win-library\"),\n",
    "                       paste0(R.version$major, \".\", R.version$minor))\n",
    "}\n",
    "dir.create(userlib, recursive = TRUE, showWarnings = FALSE)\n",
    ".libPaths(c(userlib, .libPaths()))\n",
    "''')\n",
    "\n",
    "# 2) Avoid interactive questions & use Windows binaries when possible\n",
    "r('options(repos=\"https://cloud.r-project.org\", pkgType=\"win.binary\", ask=FALSE)')\n",
    "\n",
    "# 3) Install prerequisites into the user library (explicit lib= to avoid prompts)\n",
    "r('install.packages(c(\"xts\",\"zoo\",\"Rcpp\",\"RcppArmadillo\",\"numDeriv\",\"Rsolnp\"), '\n",
    "  'lib=.libPaths()[1])')\n",
    "\n",
    "# 4) Install rugarch\n",
    "r('install.packages(\"rugarch\", lib=.libPaths()[1])')\n",
    "\n",
    "# 5) Verify\n",
    "print(\"R home   :\", r('R.home()')[0])\n",
    "print(\"Lib paths:\", list(r('.libPaths()')))\n",
    "print(\"rugarch  :\", r('as.character(packageVersion(\"rugarch\"))')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8a742-4328-4a70-9b44-f1447513634d",
   "metadata": {},
   "source": [
    "# Converters + quick rugarch import check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca43e68-860e-45a3-99b8-c0fc271a887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\n",
      "Trying to import in ABI mode.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ro.conversion.rpy2py(obj)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Smoke-load 'rugarch' so we fail early if it's missing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mro\u001b[49m.r(\u001b[33m'\u001b[39m\u001b[33msuppressMessages(library(rugarch))\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mrugarch loaded OK\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ro' is not defined"
     ]
    }
   ],
   "source": [
    "# === 2) Converters + quick 'rugarch' import check ===\n",
    "# - Defines to_r / from_r conversion helpers (no global activate)\n",
    "# - Verifies that 'rugarch' loads inside your R\n",
    "\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "def to_r(obj):\n",
    "    \"\"\"Python -> R with pandas DataFrame/Series support (scoped).\"\"\"\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        return ro.conversion.py2rpy(obj)\n",
    "\n",
    "def from_r(obj):\n",
    "    \"\"\"R -> Python with pandas DataFrame/Series support (scoped).\"\"\"\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        return ro.conversion.rpy2py(obj)\n",
    "\n",
    "# Smoke-load 'rugarch' so we fail early if it's missing\n",
    "ro.r('suppressMessages(library(rugarch))')\n",
    "print(\"rugarch loaded OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde028a3-2676-46d0-9467-8db809ecc082",
   "metadata": {},
   "source": [
    "# Call FIEGARCH via R (rugarch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b616351-bcbd-43cd-8b04-74fc0918677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3) FIEGARCH(1,d,1)-t via R (rugarch) callable from Python ===\n",
    "# Fit FIEGARCH on ARIMAX residuals. If the FIEGARCH spec fails, it will\n",
    "# gracefully fall back to FIGARCH(1,d,1) so you always get a result.\n",
    "#\n",
    "# Inputs\n",
    "#   resid   : pd.Series (ARIMAX residuals, indexed)\n",
    "#   h_ahead : int, forecast horizon (e.g., len(val_df))\n",
    "#   var_exog: optional pd.DataFrame of variance exogenous regressors\n",
    "#             (columns like ['DXY_ret','WTI_ret']) aligned to resid.index\n",
    "#   dist    : \"std\" (Student-t), \"norm\", \"ged\", ...\n",
    "#\n",
    "# Returns dict:\n",
    "#   aic, bic, loglik, d, gamma (may be NaN if not estimated), nu,\n",
    "#   var_fc (pd.Series of sigma^2 forecasts length h_ahead),\n",
    "#   std_resid (pd.Series, standardized resid on the input window),\n",
    "#   coef (pd.Series of R coefficient vector),\n",
    "#   used_model (\"FIEGARCH\" or \"FIGARCH\"), r_warnings (list of strings)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fit_fiegarch_r(resid, h_ahead, var_exog=None, dist=\"std\"):\n",
    "    # ---- prep inputs\n",
    "    y = pd.Series(resid).dropna().astype(float)\n",
    "    ro.globalenv.clear()\n",
    "    ro.globalenv[\"y\"] = to_r(y.to_frame(\"y\"))\n",
    "\n",
    "    if isinstance(var_exog, pd.DataFrame):\n",
    "        X = var_exog.loc[y.index].astype(float)\n",
    "        ro.globalenv[\"X\"] = to_r(X)\n",
    "    else:\n",
    "        ro.globalenv[\"X\"] = ro.NULL\n",
    "\n",
    "    ro.globalenv[\"h\"]    = ro.IntVector([int(h_ahead)])\n",
    "    ro.globalenv[\"dist\"] = ro.StrVector([dist])\n",
    "\n",
    "    # ---- R code (tries FIEGARCH first; if it errors, tries FIGARCH)\n",
    "    r_code = \"\"\"\n",
    "    suppressMessages(library(rugarch))\n",
    "\n",
    "    yy   <- y$y\n",
    "    Xmat <- if (is.null(X)) NULL else as.matrix(X)\n",
    "\n",
    "    mk_spec <- function(use_fiegarch = TRUE) {\n",
    "      if (use_fiegarch) {\n",
    "        ugarchspec(\n",
    "          variance.model = list(model=\"fiGARCH\", submodel=\"FIEGARCH\",\n",
    "                                garchOrder=c(1,1), external.regressors=Xmat),\n",
    "          mean.model     = list(armaOrder=c(0,0), include.mean=FALSE),\n",
    "          distribution.model = dist\n",
    "        )\n",
    "      } else {\n",
    "        ugarchspec(\n",
    "          variance.model = list(model=\"fiGARCH\", submodel=\"FIGARCH\",\n",
    "                                garchOrder=c(1,1), external.regressors=Xmat),\n",
    "          mean.model     = list(armaOrder=c(0,0), include.mean=FALSE),\n",
    "          distribution.model = dist\n",
    "        )\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Try FIEGARCH, then fall back to FIGARCH on error\n",
    "    warn_buf <- character(0)\n",
    "    withCallingHandlers({\n",
    "      spec_try <- try(mk_spec(TRUE), silent=TRUE)\n",
    "      if (inherits(spec_try, \"try-error\")) {\n",
    "        spec <- mk_spec(FALSE)\n",
    "        used <- \"FIGARCH\"\n",
    "      } else {\n",
    "        spec <- spec_try\n",
    "        used <- \"FIEGARCH\"\n",
    "      }\n",
    "      fit <- ugarchfit(spec = spec, data = yy, solver = \"hybrid\")\n",
    "      fc  <- ugarchforecast(fit, n.ahead = h[1])\n",
    "    },\n",
    "    warning = function(w) { warn_buf <<- c(warn_buf, conditionMessage(w)); invokeRestart(\"muffleWarning\") }\n",
    "    )\n",
    "\n",
    "    cf  <- coef(fit); nms <- names(cf)\n",
    "\n",
    "    # pick out fractional d, leverage gamma (if present), and nu/shape\n",
    "    d_idx  <- grep(\"delta|longMem|d\\\\[\", nms, ignore.case=TRUE)\n",
    "    g_idx  <- grep(\"^gamma\", nms, ignore.case=TRUE)\n",
    "    nu_idx <- grep(\"^shape$|^nu$|^shape1$\", nms, ignore.case=TRUE)\n",
    "\n",
    "    d_val  <- if (length(d_idx)>0)  cf[d_idx[1]] else NA_real_\n",
    "    g_val  <- if (length(g_idx)>0)  cf[g_idx[1]] else NA_real_\n",
    "    nu_val <- if (length(nu_idx)>0) cf[nu_idx[1]] else NA_real_\n",
    "\n",
    "    list(\n",
    "      aic       = infocriteria(fit)[1],\n",
    "      bic       = infocriteria(fit)[2],\n",
    "      loglik    = likelihood(fit),\n",
    "      d         = as.numeric(d_val),\n",
    "      gamma     = as.numeric(g_val),\n",
    "      nu        = as.numeric(nu_val),\n",
    "      sigma2_fc = as.numeric(sigma(fc)^2),\n",
    "      std_resid = as.numeric(residuals(fit, standardize=TRUE)),\n",
    "      coef      = cf,\n",
    "      used      = used,\n",
    "      warnings  = warn_buf\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    res = ro.r(r_code)\n",
    "\n",
    "    # ---- pull back results\n",
    "    aic   = float(res.rx2(\"aic\")[0])\n",
    "    bic   = float(res.rx2(\"bic\")[0])\n",
    "    ll    = float(res.rx2(\"loglik\")[0])\n",
    "    d_hat = float(res.rx2(\"d\")[0])      if len(res.rx2(\"d\"))      else np.nan\n",
    "    gamma = float(res.rx2(\"gamma\")[0])  if len(res.rx2(\"gamma\"))  else np.nan\n",
    "    nu    = float(res.rx2(\"nu\")[0])     if len(res.rx2(\"nu\"))     else np.nan\n",
    "    used  = str(res.rx2(\"used\")[0])     if len(res.rx2(\"used\"))   else \"FIEGARCH\"\n",
    "\n",
    "    var_fc    = pd.Series(from_r(res.rx2(\"sigma2_fc\")), name=\"var_fc_fi\")\n",
    "    std_resid = pd.Series(from_r(res.rx2(\"std_resid\")), index=y.index, name=\"std_resid_fi\")\n",
    "\n",
    "    coef_r = res.rx2(\"coef\")\n",
    "    coef   = pd.Series(from_r(coef_r), index=list(coef_r.names))\n",
    "\n",
    "    r_warnings = list(from_r(res.rx2(\"warnings\"))) if len(res.rx2(\"warnings\")) else []\n",
    "\n",
    "    return dict(aic=aic, bic=bic, loglik=ll, d=d_hat, gamma=gamma, nu=nu,\n",
    "                var_fc=var_fc, std_resid=std_resid, coef=coef,\n",
    "                used_model=used, r_warnings=r_warnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa50ec-6908-4eef-bb45-b353ce48d4eb",
   "metadata": {},
   "source": [
    "# Rebuild ARIMAX residuals + define val_df (fixed order, auto column-finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b880b-78e7-45c2-9e80-c108690ec9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4) Rebuild ARIMAX residuals + define val_df (fixed order 2,0,3) ===\n",
    "# Requirements in df (either existing returns or price columns we can turn to returns):\n",
    "#   - SPY log returns  -> 'SPY_log_ret'  (else inferred from price like 'SPY_Close')\n",
    "#   - Dollar index ret -> 'DXY_ret'      (else inferred from price like 'DXY_Close' or 'DX-Y.NYB')\n",
    "#   - WTI oil returns  -> 'WTI_ret'      (else inferred from price like 'WTI_Close' or 'CL=F')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "assert 'df' in globals(), \"I need your dataframe `df` in memory.\"\n",
    "df = df.copy()\n",
    "\n",
    "# ---- 4.1 find likely price columns (robust, case-insensitive)\n",
    "def find_candidates(df, include_any, include_close=True, top_n=8):\n",
    "    kws = [k.lower() for k in include_any]\n",
    "    price_kws = ['close','adj','last','px','price']\n",
    "    scored = []\n",
    "    for c in df.columns:\n",
    "        cl = str(c).lower()\n",
    "        # basic token match\n",
    "        name_score = sum(1 for k in kws if k in cl)\n",
    "        if name_score == 0:\n",
    "            continue\n",
    "        if include_close and not any(pk in cl for pk in price_kws):\n",
    "            continue\n",
    "        # extra points for exact-ish patterns\n",
    "        if re.search(r'\\bspy\\b', cl): name_score += 2\n",
    "        if re.search(r'\\bdxy\\b', cl): name_score += 2\n",
    "        if 'cl=f' in cl:             name_score += 2\n",
    "        scored.append((name_score, c))\n",
    "    scored.sort(key=lambda x: (-x[0], str(x[1])))\n",
    "    return [c for _, c in scored[:top_n]]\n",
    "\n",
    "spy_price_cands = find_candidates(df, include_any=['spy','s&p','s and p','sp500'])\n",
    "dxy_price_cands = find_candidates(df, include_any=['dxy','dx-y','usdx','dollar index','us dollar'])\n",
    "wti_price_cands = find_candidates(df, include_any=['wti','cl=f','crude','oil','wti crude'])\n",
    "\n",
    "print(\"\\nDetected price column candidates:\")\n",
    "print(\"SPY:\", spy_price_cands or \"(none)\")\n",
    "print(\"DXY:\", dxy_price_cands or \"(none)\")\n",
    "print(\"WTI:\", wti_price_cands or \"(none)\")\n",
    "\n",
    "# Manual overrides (set a column name here if auto-pick is not what you want)\n",
    "MANUAL_SPY = None  # e.g. \"SPY_Close\"\n",
    "MANUAL_DXY = None  # e.g. \"DXY_Close\" or \"DX-Y.NYB\"\n",
    "MANUAL_WTI = None  # e.g. \"CL=F\" or \"WTI_Close\"\n",
    "\n",
    "c_spy_px = MANUAL_SPY or (spy_price_cands[0] if spy_price_cands else None)\n",
    "c_dxy_px = MANUAL_DXY or (dxy_price_cands[0] if dxy_price_cands else None)\n",
    "c_wti_px = MANUAL_WTI or (wti_price_cands[0] if wti_price_cands else None)\n",
    "\n",
    "# ---- 4.2 ensure we have (or build) SPY_log_ret, DXY_ret, WTI_ret\n",
    "def make_log_ret(series):\n",
    "    s = pd.to_numeric(series, errors='coerce')\n",
    "    return np.log(s / s.shift(1)) * 100\n",
    "\n",
    "def ensure_return(df, out_col, ret_aliases, px_col, must=True):\n",
    "    # use an existing column if any alias is present\n",
    "    for al in ret_aliases:\n",
    "        if al in df.columns:\n",
    "            df[out_col] = pd.to_numeric(df[al], errors='coerce')\n",
    "            return df\n",
    "    # else build from price column\n",
    "    if px_col is not None and px_col in df.columns:\n",
    "        df[out_col] = make_log_ret(df[px_col])\n",
    "        return df\n",
    "    if must:\n",
    "        raise AssertionError(\n",
    "            f\"Could not find or build {out_col}. \"\n",
    "            f\"Set MANUAL_* to a valid price column or verify df columns.\"\n",
    "        )\n",
    "    return df\n",
    "\n",
    "spy_ret_alias = ['SPY_log_ret','SPY_ret','ret_spy','y_1d','y_t','SPY_Returns']\n",
    "dxy_ret_alias = ['DXY_ret','DX_ret','DXY_Returns','Dollar_ret','USDX_ret']\n",
    "wti_ret_alias = ['WTI_ret','CL_ret','WTI_Returns','OIL_ret','Crude_ret']\n",
    "\n",
    "df = ensure_return(df, 'SPY_log_ret', spy_ret_alias, c_spy_px, must=True)\n",
    "df = ensure_return(df, 'DXY_ret',     dxy_ret_alias, c_dxy_px, must=True)\n",
    "df = ensure_return(df, 'WTI_ret',     wti_ret_alias, c_wti_px, must=True)\n",
    "\n",
    "print(\"\\nHead of constructed/confirmed returns:\")\n",
    "print(df[['SPY_log_ret','DXY_ret','WTI_ret']].dropna().head())\n",
    "\n",
    "# ---- 4.3 split and fit ARIMAX(2,0,3) on train, keep residuals for volatility model\n",
    "df = df.sort_index()\n",
    "try:\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "n = len(df)\n",
    "i_tr = int(n*0.8); i_va = int(n*0.9)\n",
    "train_df = df.iloc[:i_tr].copy()\n",
    "val_df   = df.iloc[i_tr:i_va].copy()\n",
    "test_df  = df.iloc[i_va:].copy()\n",
    "\n",
    "y_tr = train_df['SPY_log_ret'].astype(float)\n",
    "X_tr = train_df[['DXY_ret','WTI_ret']].astype(float)\n",
    "\n",
    "order = (2, 0, 3)\n",
    "sm_model = SARIMAX(y_tr, exog=X_tr, order=order,\n",
    "                   enforce_stationarity=False, enforce_invertibility=False)\n",
    "sm_res = sm_model.fit(disp=False)\n",
    "\n",
    "resid_for_vol = pd.Series(sm_res.resid, index=y_tr.index, name='resid_tr')\n",
    "arimax_out = {'order': order, 'aic': sm_res.aic, 'resid_tr': resid_for_vol}\n",
    "\n",
    "print(f\"\\nARIMAX (fixed) order: {order} | AIC: {sm_res.aic:.2f} | resid shape: {resid_for_vol.shape}\")\n",
    "print(\"train/val/test ranges:\",\n",
    "      f\"train {train_df.index.min()} â†’ {train_df.index.max()} |\",\n",
    "      f\"val {val_df.index.min()} â†’ {val_df.index.max()} |\",\n",
    "      f\"test {test_df.index.min()} â†’ {test_df.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95be0b-7e8e-4415-b853-76ae3b0b5d90",
   "metadata": {},
   "source": [
    "# Quick Smoke Test: Verify FIEGARCH Pipeline (rugarch via rpy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a16e63-4b4e-4dad-be2a-e27933ce5cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m y = pd.Series(np.random.randn(\u001b[32m200\u001b[39m))\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Register in R environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mro\u001b[49m.globalenv.clear()\n\u001b[32m     15\u001b[39m ro.globalenv[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m] = to_r(y.to_frame(\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Run minimal FIEGARCH spec (fiGARCH/FIEGARCH)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'ro' is not defined"
     ]
    }
   ],
   "source": [
    "# === 5) Quick Smoke Test: verify Râ†’Python FIEGARCH integration ===\n",
    "# Purpose:\n",
    "#   - Confirm Pythonâ†”R bridge works\n",
    "#   - Confirm rugarch FIEGARCH spec compiles\n",
    "#   - Confirm ugarchfit() completes on synthetic data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Tiny synthetic test sample\n",
    "y = pd.Series(np.random.randn(200))\n",
    "\n",
    "# Register in R environment\n",
    "ro.globalenv.clear()\n",
    "ro.globalenv['y'] = to_r(y.to_frame('y'))\n",
    "\n",
    "# Run minimal FIEGARCH spec (fiGARCH/FIEGARCH)\n",
    "print(\"Running FIEGARCH smoke test via rugarch...\")\n",
    "\n",
    "ro.r('''\n",
    "suppressMessages(library(rugarch))\n",
    "yy <- y$y\n",
    "\n",
    "spec <- ugarchspec(\n",
    "  variance.model = list(model=\"fiGARCH\", submodel=\"FIEGARCH\",\n",
    "                        garchOrder=c(1,1)),\n",
    "  mean.model     = list(armaOrder=c(0,0), include.mean=FALSE),\n",
    "  distribution.model = \"std\"\n",
    ")\n",
    "\n",
    "fit <- ugarchfit(spec=spec, data=yy, solver=\"hybrid\")\n",
    "print(fit)\n",
    "''')\n",
    "\n",
    "print(\"âœ… FIEGARCH smoke test completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e9720-f202-44df-a55c-cbed165832b0",
   "metadata": {},
   "source": [
    "# Full FIEGARCH Fit Call (using ARIMAX residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6e4d7-ea00-4890-9e81-c8133cee22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6) ULTRA-SAFE FIEGARCH(1,d,1)-t fit using ARIMAX residuals (FIGARCH fallback) ===\n",
    "# Run this right after the smoke test. If itâ€™s stable, you can relax the caps later.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "# --- Keep R to 1 thread (inside R). We do NOT touch Python threads here to avoid clashes.\n",
    "ro.r(r\"\"\"\n",
    "suppressMessages({\n",
    "  if (requireNamespace(\"RhpcBLASctl\", quietly=TRUE)) {\n",
    "    RhpcBLASctl::blas_set_num_threads(1)\n",
    "    RhpcBLASctl::omp_set_num_threads(1)\n",
    "  }\n",
    "})\n",
    "\"\"\")\n",
    "\n",
    "# --- Prepare residuals: percent -> decimals, drop non-finite, soft-clip extreme tails\n",
    "resid_full = pd.to_numeric(arimax_out['resid_tr'], errors='coerce').dropna() / 100.0\n",
    "if not np.isfinite(resid_full).all():\n",
    "    resid_full = resid_full[np.isfinite(resid_full)]\n",
    "\n",
    "# soft clip to +/- 8 * std (rugarch can blow up with extreme outliers on first fit)\n",
    "s = resid_full.std()\n",
    "if s > 0:\n",
    "    resid_full = resid_full.clip(lower=-8*s, upper=8*s)\n",
    "\n",
    "# Start with a small window; increase after stability (e.g., 2000 -> 4000 -> 9000)\n",
    "MAX_N = 2000\n",
    "resid_dec = resid_full.iloc[-min(MAX_N, len(resid_full)) : ].copy()\n",
    "\n",
    "# Forecast horizon: start small; you can switch to len(val_df) after stability\n",
    "h_va_cap = 30\n",
    "h_va = int(min(len(val_df), h_va_cap, max(1, len(resid_dec)//10)))  # simple guard\n",
    "\n",
    "# No variance exogenous regressors on the first stable run\n",
    "var_exog = None\n",
    "\n",
    "# ---- Run the fit via your bridge, but ask it to return R warnings and fallback info.\n",
    "# To get warnings & model used, we pass through via a slightly safer R wrapper.\n",
    "r_code = \"\"\"\n",
    "function(y, h, X=NULL, dist=\"std\") {\n",
    "  suppressMessages(library(rugarch))\n",
    "  set.seed(123)\n",
    "\n",
    "  yy   <- as.numeric(y)\n",
    "  Xmat <- if (is.null(X)) NULL else as.matrix(X)\n",
    "\n",
    "  warns <- character()\n",
    "\n",
    "  mk_spec <- function(use_fiegarch=TRUE) {\n",
    "    if (use_fiegarch) {\n",
    "      ugarchspec(\n",
    "        variance.model = list(model=\"fGARCH\", submodel=\"FIEGARCH\",\n",
    "                              garchOrder=c(1,1), external.regressors=Xmat),\n",
    "        mean.model     = list(armaOrder=c(0,0), include.mean=FALSE),\n",
    "        distribution.model = dist\n",
    "      )\n",
    "    } else {\n",
    "      ugarchspec(\n",
    "        variance.model = list(model=\"fGARCH\", submodel=\"FIGARCH\",\n",
    "                              garchOrder=c(1,1), external.regressors=Xmat),\n",
    "        mean.model     = list(armaOrder=c(0,0), include.mean=FALSE),\n",
    "        distribution.model = dist\n",
    "      )\n",
    "    }\n",
    "  }\n",
    "\n",
    "  fit_once <- function(spec) {\n",
    "    withCallingHandlers({\n",
    "        fit <- ugarchfit(\n",
    "          spec = spec, data = yy,\n",
    "          solver = \"hybrid\",\n",
    "          fit.control = list(scale = TRUE, stationarity = 1),\n",
    "          solver.control = list(trace = 0)\n",
    "        )\n",
    "        fit\n",
    "      },\n",
    "      warning=function(w) { warns <<- c(warns, conditionMessage(w)); invokeRestart(\"muffleWarning\") }\n",
    "    )\n",
    "  }\n",
    "\n",
    "  # Try FIEGARCH first, then FIGARCH if needed\n",
    "  used_model <- \"FIEGARCH\"\n",
    "  fit <- try(fit_once(mk_spec(TRUE)), silent=TRUE)\n",
    "  if (inherits(fit, \"try-error\")) {\n",
    "    used_model <- \"FIGARCH\"\n",
    "    fit <- fit_once(mk_spec(FALSE))\n",
    "  }\n",
    "\n",
    "  fc <- ugarchforecast(fit, n.ahead = h)\n",
    "\n",
    "  cf  <- coef(fit); nms <- names(cf)\n",
    "  d_idx  <- grep(\"delta|longMem|d\\\\[\", nms, ignore.case=TRUE)\n",
    "  g_idx  <- grep(\"^gamma\", nms, ignore.case=TRUE)\n",
    "  nu_idx <- grep(\"^shape$|^nu$|^shape1$\", nms, ignore.case=TRUE)\n",
    "\n",
    "  d_val  <- if (length(d_idx)>0)  cf[d_idx[1]] else NA_real_\n",
    "  g_val  <- if (length(g_idx)>0)  cf[g_idx[1]] else NA_real_\n",
    "  nu_val <- if (length(nu_idx)>0) cf[nu_idx[1]] else NA_real_\n",
    "\n",
    "  list(\n",
    "    aic       = infocriteria(fit)[1],\n",
    "    bic       = infocriteria(fit)[2],\n",
    "    loglik    = likelihood(fit),\n",
    "    d         = as.numeric(d_val),\n",
    "    gamma     = as.numeric(g_val),\n",
    "    nu        = as.numeric(nu_val),\n",
    "    sigma2_fc = as.numeric(sigma(fc)^2),\n",
    "    std_resid = as.numeric(residuals(fit, standardize=TRUE)),\n",
    "    coef      = cf,\n",
    "    used_model= used_model,\n",
    "    r_warnings= warns\n",
    "  )\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build the R wrapper once\n",
    "r_fit_wrapper = ro.r(r_code)\n",
    "\n",
    "# Call it\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    r_y  = ro.conversion.py2rpy(resid_dec.to_frame(\"y\"))\n",
    "    r_X  = ro.NULL  # var_exog=None for now\n",
    "    res  = r_fit_wrapper(r_y.rx2(\"y\"), h_va, r_X, \"std\")\n",
    "\n",
    "# Pull back results\n",
    "def rvec_to_series(x, name=None, index=None):\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        s = ro.conversion.rpy2py(x)\n",
    "    if isinstance(s, pd.Series):\n",
    "        if name is not None: s.name = name\n",
    "        if index is not None and len(index) == len(s): s.index = index\n",
    "        return s\n",
    "    return pd.Series(s, index=index, name=name)\n",
    "\n",
    "aic   = float(res.rx2(\"aic\")[0])\n",
    "bic   = float(res.rx2(\"bic\")[0])\n",
    "ll    = float(res.rx2(\"loglik\")[0])\n",
    "d_hat = float(res.rx2(\"d\")[0]) if len(res.rx2(\"d\")) else np.nan\n",
    "gamma = float(res.rx2(\"gamma\")[0]) if len(res.rx2(\"gamma\")) else np.nan\n",
    "nu    = float(res.rx2(\"nu\")[0]) if len(res.rx2(\"nu\")) else np.nan\n",
    "used_model = str(res.rx2(\"used_model\")[0])\n",
    "warns = list(ro.vectors.ListVector(res).rx2(\"r_warnings\"))\n",
    "\n",
    "var_fc    = rvec_to_series(res.rx2(\"sigma2_fc\"), name=\"var_fc_fiegarch\")\n",
    "std_resid = rvec_to_series(res.rx2(\"std_resid\"), index=resid_dec.index, name=\"std_resid_fiegarch\")\n",
    "\n",
    "coef_r = res.rx2(\"coef\")\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    coef = pd.Series(ro.conversion.rpy2py(coef_r), index=list(coef_r.names))\n",
    "\n",
    "# Align variance forecast to validation index (short horizon for the very first run)\n",
    "var_fc_fiegarch = var_fc.set_axis(val_df.index[:len(var_fc)])\n",
    "\n",
    "print(f\"\\nModel used : {used_model}\")\n",
    "print(f\"Sample     : {len(resid_dec)} (of total {len(resid_full)}) | h={len(var_fc)}\")\n",
    "print(f\"AIC={aic:.3f}, BIC={bic:.3f}, logLik={ll:.2f}, d={d_hat:.3f}, gamma={gamma:.3f}, nu={nu:.2f}\")\n",
    "if warns:\n",
    "    print(\"\\nâ–² R warnings:\")\n",
    "    for w in warns:\n",
    "        print(\" -\", w)\n",
    "\n",
    "# Pack for later steps\n",
    "fiegarch_out = {\n",
    "    \"aic\": aic, \"bic\": bic, \"loglik\": ll, \"d\": d_hat, \"gamma\": gamma, \"nu\": nu,\n",
    "    \"var_fc\": var_fc_fiegarch, \"std_resid\": std_resid, \"coef\": coef, \"used_model\": used_model,\n",
    "    \"r_warnings\": warns\n",
    "}\n",
    "\n",
    "print(\"\\nVariance forecast (head):\")\n",
    "print(var_fc_fiegarch.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63dcd07-4b39-4bf8-8377-aa2d71e2bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Build & save FULL AWT + 21-indicator + exogenous dataset\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"data/processed\")\n",
    "BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Build SPY core AWT + indicators + y_1d from raw SPY OHLCV\n",
    "#    Use the original Yahoo SPY frame, but flatten its MultiIndex columns.\n",
    "df_spy_raw_flat = flatten_cols(df_spx_raw.copy())\n",
    "print(\"Flattened SPY columns:\", df_spy_raw_flat.columns)\n",
    "\n",
    "df_awt_core = compute_awt_features(df_spy_raw_flat)\n",
    "print(\"Core AWT shape (SPY only):\", df_awt_core.shape)\n",
    "\n",
    "# 2) Exogenous returns (DXY, WTI) from your merged & cleaned dataframe `df`\n",
    "#    `df` is from the \"Data Preprocessing & Transformation\" section.\n",
    "#    Align them explicitly to the SPY index.\n",
    "df_exog = df[['DXY_ret', 'WTI_ret']].copy()\n",
    "df_exog = df_exog.reindex(df_awt_core.index)\n",
    "\n",
    "print(\"Exog columns:\", df_exog.columns)\n",
    "print(\"Exog shape (reindexed):\", df_exog.shape)\n",
    "\n",
    "# 3) Combine core AWT + exogenous on the Date index\n",
    "#    Use concat (same effect as a left join on the index).\n",
    "df_awt = pd.concat([df_awt_core, df_exog], axis=1)\n",
    "\n",
    "# Only require that the target exists; keep rows even if some indicators/exog have NaNs\n",
    "df_awt = df_awt[df_awt['y_1d'].notna()]\n",
    "\n",
    "print(\"Final AWT + exogenous + target shape:\", df_awt.shape)\n",
    "\n",
    "# 4) Save\n",
    "out_path = BASE / \"df_awt_features.csv\"\n",
    "df_awt.to_csv(out_path)\n",
    "print(\"âœ… Saved AWT + 21 indicators + exogenous + target to:\", out_path.resolve())\n",
    "print(df_awt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37633052-7283-4f9b-ae43-5abae2d95d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 3ï¸âƒ£  Compute AWT + Features + Target\n",
    "# ================================================================\n",
    "# ------------------------------------------------\n",
    "# Robust AWT + features + target (auto-make SPY_log_ret if missing)\n",
    "# ------------------------------------------------\n",
    "def _first_present(colnames, df_cols):\n",
    "    for c in colnames:\n",
    "        if c in df_cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def compute_awt_features(df_slice: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build AWT-based features + 21 indicators safely.\n",
    "    Requires SPY OHLCV if available; will fall back where possible.\n",
    "    Produces:\n",
    "      SPY_s, SPY_d, and 21 indicators incl. MACD, RSI, ADX, OBV, MFI, WVAD, etc.\n",
    "    \"\"\"\n",
    "    # -------- helpers --------\n",
    "    def _flatten(df):\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = ['_'.join([str(c) for c in tup if c is not None]).strip()\n",
    "                          for tup in df.columns]\n",
    "        else:\n",
    "            df.columns = [str(c).strip() for c in df.columns]\n",
    "        return df.loc[:, ~pd.Index(df.columns).duplicated()]\n",
    "\n",
    "    def _find_col(df, *must_contain):\n",
    "        must = [m.lower() for m in must_contain]\n",
    "        for c in df.columns:\n",
    "            lc = str(c).lower()\n",
    "            if all(m in lc for m in must):\n",
    "                return c\n",
    "        # simple fallbacks for common single-level names\n",
    "        lut = {\n",
    "            ('spy', 'open'):   ['SPY_Open', 'Open', 'Open_SPY'],\n",
    "            ('spy', 'high'):   ['SPY_High', 'High', 'High_SPY'],\n",
    "            ('spy', 'low'):    ['SPY_Low', 'Low', 'Low_SPY'],\n",
    "            ('spy', 'close'):  ['SPY_Close', 'Close', 'Close_SPY'],\n",
    "            ('spy', 'volume'): ['SPY_Volume', 'Volume', 'Volume_SPY'],\n",
    "        }\n",
    "        for k, alts in lut.items():\n",
    "            if set(k) == set(must):\n",
    "                for a in alts:\n",
    "                    if a in df.columns:\n",
    "                        return a\n",
    "        return None\n",
    "\n",
    "    def _wavelet_denoise(x, wavelet=\"db2\", level=2, mode=\"soft\"):\n",
    "        x = pd.Series(x).astype(float)\n",
    "        coeffs = pywt.wavedec(x.values, wavelet=wavelet, level=level)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745 if len(coeffs[-1]) else 0.0\n",
    "        thr   = sigma * np.sqrt(2 * np.log(len(x)))\n",
    "        coeffs_thr = [coeffs[0]] + [pywt.threshold(c, thr, mode=mode) for c in coeffs[1:]]\n",
    "        x_s = pywt.waverec(coeffs_thr, wavelet)[:len(x)]\n",
    "        x_d = x.values[:len(x_s)] - x_s\n",
    "        return pd.DataFrame({\"SPY_s\": x_s, \"SPY_d\": x_d}, index=x.index)\n",
    "\n",
    "    # -------- start --------\n",
    "    df_f = _flatten(df_slice.copy())\n",
    "\n",
    "    # Locate OHLCV columns (best effort)\n",
    "    c_open   = _find_col(df_f, 'spy', 'open')\n",
    "    c_high   = _find_col(df_f, 'spy', 'high')\n",
    "    c_low    = _find_col(df_f, 'spy', 'low')\n",
    "    c_close  = _find_col(df_f, 'spy', 'close')\n",
    "    c_volume = _find_col(df_f, 'spy', 'volume')\n",
    "\n",
    "    # Ensure log returns exist (needed for AWT)\n",
    "    if 'SPY_log_ret' not in df_f.columns:\n",
    "        if c_close is None:\n",
    "            raise KeyError(\"compute_awt_features() needs SPY close prices to derive SPY_log_ret.\")\n",
    "        df_f['SPY_log_ret'] = np.log(df_f[c_close] / df_f[c_close].shift(1)) * 100\n",
    "\n",
    "    # AWT on SPY_log_ret -> SPY_s (smooth), SPY_d (detail)\n",
    "    s_hat = _wavelet_denoise(df_f['SPY_log_ret'])\n",
    "    df_f = df_f.join(s_hat, how='left')\n",
    "\n",
    "    # Choose price series for TA (prefer denoised)\n",
    "    price = df_f['SPY_s'].fillna(df_f['SPY_log_ret'])\n",
    "\n",
    "    # ---------------- 21 INDICATORS ----------------\n",
    "    # 1â€“3 Trend MAs\n",
    "    df_f['EMA20'] = ta.trend.EMAIndicator(close=price, window=20).ema_indicator()\n",
    "    df_f['MA5']   = ta.trend.SMAIndicator(close=price, window=5).sma_indicator()\n",
    "    df_f['MA10']  = ta.trend.SMAIndicator(close=price, window=10).sma_indicator()\n",
    "\n",
    "    # 4 MACD\n",
    "    df_f['MACD']  = ta.trend.MACD(close=price).macd()\n",
    "\n",
    "    # 5 ROC(10)\n",
    "    df_f['ROC']   = ta.momentum.ROCIndicator(close=price, window=10).roc()\n",
    "\n",
    "    # 6â€“7 Momentum (6m, 12m)\n",
    "    df_f['MTM6']  = price / price.shift(126) - 1\n",
    "    df_f['MTM12'] = price / price.shift(252) - 1\n",
    "\n",
    "    # 8 SMI (stochastic momentum index)\n",
    "    if c_high and c_low:\n",
    "        stoch = ta.momentum.StochasticOscillator(high=df_f[c_high], low=df_f[c_low],\n",
    "                                                 close=price, window=14, smooth_window=3)\n",
    "        df_f['SMI'] = stoch.stoch_signal()\n",
    "    else:\n",
    "        df_f['SMI'] = np.nan\n",
    "\n",
    "    # 9 ATR(14)\n",
    "    df_f['ATR'] = (ta.volatility.AverageTrueRange(\n",
    "        high=df_f[c_high] if c_high else price,\n",
    "        low=df_f[c_low] if c_low else price,\n",
    "        close=price, window=14\n",
    "    ).average_true_range())\n",
    "\n",
    "    # 10â€“12 Bollinger\n",
    "    boll = ta.volatility.BollingerBands(close=price, window=20, window_dev=2)\n",
    "    df_f['BOLL_upper']     = boll.bollinger_hband()\n",
    "    df_f['BOLL_lower']     = boll.bollinger_lband()\n",
    "    df_f['BOLL_bandwidth'] = df_f['BOLL_upper'] - df_f['BOLL_lower']\n",
    "\n",
    "    # 13 CCI(20)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['CCI'] = ta.trend.CCIIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], window=20\n",
    "        ).cci()\n",
    "    else:\n",
    "        df_f['CCI'] = np.nan\n",
    "\n",
    "    # 14 WVAD (needs OHLCV)\n",
    "    if c_open and c_high and c_low and c_close and c_volume:\n",
    "        rng = (df_f[c_high] - df_f[c_low]).replace(0, np.nan)\n",
    "        df_f['WVAD'] = ((df_f[c_close] - df_f[c_open]) / (rng + 1e-10)) * df_f[c_volume]\n",
    "    else:\n",
    "        df_f['WVAD'] = np.nan\n",
    "\n",
    "    # 15 RSI(14)\n",
    "    df_f['RSI14'] = ta.momentum.RSIIndicator(close=price, window=14).rsi()\n",
    "\n",
    "    # 16â€“17 Stochastic %K/%D (classic)\n",
    "    if c_high and c_low:\n",
    "        st = ta.momentum.StochasticOscillator(high=df_f[c_high], low=df_f[c_low],\n",
    "                                              close=price, window=14, smooth_window=3)\n",
    "        df_f['STO_K'] = st.stoch()\n",
    "        df_f['STO_D'] = st.stoch_signal()\n",
    "    else:\n",
    "        df_f[['STO_K', 'STO_D']] = np.nan\n",
    "\n",
    "    # 18 ADX(14)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['ADX14'] = ta.trend.ADXIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], window=14\n",
    "        ).adx()\n",
    "    else:\n",
    "        df_f['ADX14'] = np.nan\n",
    "\n",
    "    # 19 OBV\n",
    "    if c_close and c_volume:\n",
    "        df_f['OBV'] = ta.volume.OnBalanceVolumeIndicator(\n",
    "            close=df_f[c_close], volume=df_f[c_volume]\n",
    "        ).on_balance_volume()\n",
    "    else:\n",
    "        df_f['OBV'] = np.nan\n",
    "\n",
    "    # 20 MFI(14)\n",
    "    if c_high and c_low and c_close and c_volume:\n",
    "        df_f['MFI14'] = ta.volume.MFIIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low],\n",
    "            close=df_f[c_close], volume=df_f[c_volume], window=14\n",
    "        ).money_flow_index()\n",
    "    else:\n",
    "        df_f['MFI14'] = np.nan\n",
    "\n",
    "    # 21 Williams %R(14)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['WILLR14'] = ta.momentum.WilliamsRIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], lbp=14\n",
    "        ).williams_r()\n",
    "    else:\n",
    "        df_f['WILLR14'] = np.nan\n",
    "\n",
    "    # Target (example: next-day log return)\n",
    "    df_f['y_1d'] = df_f['SPY_log_ret'].shift(-1)\n",
    "\n",
    "    # --- Final clean ---\n",
    "    # Only require that the target exists; keep rows even if some indicators have NaNs.\n",
    "    # This avoids dropping the entire dataset.\n",
    "    df_f = df_f[df_f['y_1d'].notna()]\n",
    "\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057aa03a-e3c7-47cd-9aa0-043314762b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Rolling 1-step-ahead forecasts (ARIMAX mean + EGARCH variance)\n",
    "# Using fixed EGARCH(2,1,1)-t and selected ARIMAX(p,d,q)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "\n",
    "# --- inputs expected from previous cells:\n",
    "# y, X  -> prepared series/dataframe\n",
    "# p, d_, q  -> selected ARIMAX orders from Cell 4\n",
    "ep, eo, eq, edist = 2, 1, 1, 't'  # fixed EGARCH config\n",
    "\n",
    "START  = int(len(y) * 0.70)       # expanding window start (70% train)\n",
    "OUTDIR = Path(\"data/processed\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mu_vals, mu_idx = [], []\n",
    "var_vals, var_idx = [], []\n",
    "\n",
    "# Optional: keep last ARIMAX params to speed convergence\n",
    "last_params = None\n",
    "\n",
    "for t in range(START, len(y)):\n",
    "    y_tr = y.iloc[:t]\n",
    "    X_tr = X.iloc[:t]\n",
    "    X_fc = X.iloc[[t]]\n",
    "\n",
    "    # --- ARIMAX mean forecast (1-step)\n",
    "    mod = SARIMAX(y_tr, order=(p, d_, q), exog=X_tr, trend='n',\n",
    "                  enforce_stationarity=False, enforce_invertibility=False)\n",
    "    # warm-start if available\n",
    "    res = mod.fit(disp=False, start_params=last_params) if last_params is not None else mod.fit(disp=False)\n",
    "    last_params = res.params.values  # cache for next iteration\n",
    "\n",
    "    mu_hat = res.get_forecast(steps=1, exog=X_fc).predicted_mean.iloc[0]\n",
    "    mu_vals.append(mu_hat); mu_idx.append(y.index[t])\n",
    "\n",
    "    # --- EGARCH variance forecast (1-step) on ARIMAX residuals\n",
    "    resid_tr = res.resid.dropna()\n",
    "    eg = arch_model(resid_tr, mean='Zero', vol='EGARCH',\n",
    "                    p=ep, o=eo, q=eq, dist=edist).fit(disp='off')\n",
    "    var_hat = eg.forecast(horizon=1, reindex=True).variance.iloc[-1, 0]\n",
    "    var_vals.append(var_hat); var_idx.append(y.index[t])\n",
    "\n",
    "# Build result series\n",
    "mu_fc  = pd.Series(mu_vals,  index=pd.DatetimeIndex(mu_idx),  name='arimax_mu_fc1')\n",
    "var_fc = pd.Series(var_vals, index=pd.DatetimeIndex(var_idx), name='egarch_var_fc1')\n",
    "\n",
    "# Save artifacts\n",
    "mu_fc.to_csv(OUTDIR / \"arimax_mu_fc1.csv\")\n",
    "var_fc.to_csv(OUTDIR / \"egarch_var_fc1.csv\")\n",
    "\n",
    "print(\"âœ… Saved:\",\n",
    "      OUTDIR / \"arimax_mu_fc1.csv\",\n",
    "      OUTDIR / \"egarch_var_fc1.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LSTM_Research_Project)",
   "language": "python",
   "name": "lstm_research_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
