{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9df2ba3b-890b-4272-9dce-9ba8bd9006ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo Finance Data Acquisition of Required Data\n",
    "#Target Period: 2006-01-01 to 2025-06-30\n",
    "\n",
    "#Imports\n",
    "#Import Yahoo Finance data library for data acquisition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "120b0669-fbe8-4962-ba2b-5e47173eb392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_33160\\3065575225.py:6: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df_spx_raw = yf.download(\"SPY\", start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_33160\\3065575225.py:9: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df_dxy = yf.download(\"DX-Y.NYB\", start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_33160\\3065575225.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df_wti = yf.download(\"CL=F\", start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price           Close       High        Low       Open    Volume\n",
      "Ticker            SPY        SPY        SPY        SPY       SPY\n",
      "Date                                                            \n",
      "2006-01-03  87.721710  87.929420  86.122366  86.676254  73256700\n",
      "2006-01-04  88.137123  88.268667  87.721704  87.832484  51899600\n",
      "2006-01-05  88.192520  88.337915  87.846341  88.033281  47307500\n",
      "2006-01-06  88.926437  89.023367  88.178690  88.635648  62885900\n",
      "2006-01-09  89.154915  89.355694  88.884896  88.912585  43527400\n",
      "Price           Close       High        Low       Open   Volume\n",
      "Ticker       DX-Y.NYB   DX-Y.NYB   DX-Y.NYB   DX-Y.NYB DX-Y.NYB\n",
      "Date                                                           \n",
      "2006-01-03  89.839996  90.940002  89.779999  90.750000        0\n",
      "2006-01-04  89.139999  89.860001  89.010002  89.730003        0\n",
      "2006-01-05  89.330002  89.629997  89.180000  89.269997        0\n",
      "2006-01-06  88.849998  89.629997  88.800003  89.349998        0\n",
      "2006-01-09  89.250000  89.449997  88.809998  88.959999        0\n",
      "Price           Close       High        Low       Open  Volume\n",
      "Ticker           CL=F       CL=F       CL=F       CL=F    CL=F\n",
      "Date                                                          \n",
      "2006-01-03  63.139999  63.799999  60.810001  61.040001  130635\n",
      "2006-01-04  63.419998  63.650002  62.259998  63.000000  105194\n",
      "2006-01-05  62.790001  63.799999  62.599998  63.400002  104035\n",
      "2006-01-06  64.209999  64.449997  62.599998  62.599998  110763\n",
      "2006-01-09  63.500000  64.610001  62.900002  64.150002  115558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Define instruments and Date range\n",
    "start = \"2006-01-01\"\n",
    "end = \"2025-06-30\"\n",
    "\n",
    "# S&P500 ETF (proxy)\n",
    "df_spx_raw = yf.download(\"SPY\", start=start, end=end)\n",
    "\n",
    "# Dollar Index (Yahoo ticker: DX-Y.NYB)\n",
    "df_dxy = yf.download(\"DX-Y.NYB\", start=start, end=end)\n",
    "\n",
    "# WTI Oil Futures (Yahoo ticker: CL=F)\n",
    "df_wti = yf.download(\"CL=F\", start=start, end=end)\n",
    "\n",
    "print(df_spx_raw.head())\n",
    "print(df_dxy.head())\n",
    "print(df_wti.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81563e-688b-46b7-b0c7-8a2382a8bf60",
   "metadata": {},
   "source": [
    "# Data Preprocesssing & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9e88553b-165d-426d-a153-e748dbc39e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY shape: (4902, 5)\n",
      "DXY shape: (4905, 5)\n",
      "WTI shape: (4901, 5)\n",
      "Merged dataframe shape: (4898, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\anaconda3\\envs\\LSTM_Research_Project\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:395: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#Load and confirm the raw datasets\n",
    "print(\"SPY shape:\", df_spx_raw.shape)\n",
    "print(\"DXY shape:\", df_dxy.shape)\n",
    "print(\"WTI shape:\", df_wti.shape)\n",
    "\n",
    "#Standardise column names and isolate 'Close' column\n",
    "df_spx = df_spx_raw[['Close']].rename(columns={'Close': 'SPY_Close'})\n",
    "df_dxy = df_dxy[['Close']].rename(columns={'Close': 'DXY_Close'})\n",
    "df_wti = df_wti[['Close']].rename(columns={'Close': 'WTI_Close'})\n",
    "\n",
    "#Merge and clean datasets\n",
    "df = df_spx.join([df_dxy, df_wti], how='inner')\n",
    "df = df.dropna()\n",
    "print(\"Merged dataframe shape:\", df.shape)\n",
    "\n",
    "#Compute returns and changes\n",
    "df['SPY_log_ret'] = np.log(df['SPY_Close'] / df['SPY_Close'].shift(1)) * 100\n",
    "df['SPY_price_change'] = df['SPY_Close'].diff()\n",
    "df['SPY_p_change'] = df['SPY_Close'].pct_change() * 100\n",
    "\n",
    "df['DXY_ret'] = np.log(df['DXY_Close'] / df['DXY_Close'].shift(1)) * 100\n",
    "df['WTI_ret'] = np.log(df['WTI_Close'] / df['WTI_Close'].shift(1)) * 100\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5c77d-0044-46bf-bc47-41987d2b4092",
   "metadata": {},
   "source": [
    "# Time Series Splitting & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8fa31-192d-4d9b-99e0-035f32e81264",
   "metadata": {},
   "source": [
    "Static Chronological Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2cafe036-c812-4db8-89ce-663351d2e7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 4895\n",
      "Train: 3916 (80.0%), Validation: 489 (10.0%), Test: 490 (10.0%)\n",
      "\n",
      "Train range: 2006-01-04 00:00:00 → 2021-08-03 00:00:00\n",
      "Validation range: 2021-08-04 00:00:00 → 2023-07-14 00:00:00\n",
      "Test range: 2023-07-17 00:00:00 → 2025-06-27 00:00:00\n",
      "\n",
      "Sample from training data:\n",
      "Price        SPY_Close  DXY_Close  WTI_Close SPY_log_ret SPY_price_change  \\\n",
      "Ticker             SPY   DX-Y.NYB       CL=F                                \n",
      "Date                                                                        \n",
      "2021-07-28  413.541992  92.320000  72.389999   -0.041022        -0.169678   \n",
      "2021-07-29  415.257141  91.860001  73.620003    0.413888         1.715149   \n",
      "2021-07-30  413.240479  92.169998  73.949997   -0.486825        -2.016663   \n",
      "2021-08-02  412.373474  92.050003  71.260002   -0.210027        -0.867004   \n",
      "2021-08-03  415.728271  92.080002  70.559998    0.810242         3.354797   \n",
      "\n",
      "Price      SPY_p_change   DXY_ret   WTI_ret  \n",
      "Ticker                                       \n",
      "Date                                         \n",
      "2021-07-28    -0.041014 -0.119081  1.027498  \n",
      "2021-07-29     0.414746 -0.499511  1.684861  \n",
      "2021-07-30    -0.485642  0.336899  0.447238  \n",
      "2021-08-02    -0.209806 -0.130274 -3.705396  \n",
      "2021-08-03     0.813534  0.032584 -0.987181  \n",
      "\n",
      "Sample from validation data:\n",
      "Price        SPY_Close  DXY_Close  WTI_Close SPY_log_ret SPY_price_change  \\\n",
      "Ticker             SPY   DX-Y.NYB       CL=F                                \n",
      "Date                                                                        \n",
      "2021-08-04  413.683319  92.269997  68.150002   -0.493110        -2.044952   \n",
      "2021-08-05  416.303192  92.239998  69.089996    0.631307         2.619873   \n",
      "2021-08-06  416.991089  92.800003  68.279999    0.165103         0.687897   \n",
      "2021-08-09  416.651794  92.949997  66.480003   -0.081400        -0.339294   \n",
      "2021-08-10  417.170166  93.059998  68.290001    0.124336         0.518372   \n",
      "\n",
      "Price      SPY_p_change   DXY_ret   WTI_ret  \n",
      "Ticker                                       \n",
      "Date                                         \n",
      "2021-08-04    -0.491896  0.206124 -3.475220  \n",
      "2021-08-05     0.633304 -0.032517  1.369877  \n",
      "2021-08-06     0.165239  0.605282 -1.179307  \n",
      "2021-08-09    -0.081367  0.161501 -2.671568  \n",
      "2021-08-10     0.124414  0.118274  2.686216  \n",
      "\n",
      "Sample from test data:\n",
      "Price        SPY_Close   DXY_Close  WTI_Close SPY_log_ret SPY_price_change  \\\n",
      "Ticker             SPY    DX-Y.NYB       CL=F                                \n",
      "Date                                                                         \n",
      "2023-07-17  437.937073   99.839996  74.150002    0.346627         1.515381   \n",
      "2023-07-18  441.191162   99.940002  75.750000    0.740303         3.254089   \n",
      "2023-07-19  442.172272  100.279999  75.349998    0.222131         0.981110   \n",
      "2023-07-20  439.238678  100.879997  75.629997   -0.665661        -2.933594   \n",
      "2023-07-21  439.238678  101.070000  77.070000    0.000000         0.000000   \n",
      "\n",
      "Price      SPY_p_change   DXY_ret   WTI_ret  \n",
      "Ticker                                       \n",
      "Date                                         \n",
      "2023-07-17     0.347229 -0.070095 -1.698238  \n",
      "2023-07-18     0.743050  0.100116  2.134836  \n",
      "2023-07-19     0.222377  0.339623 -0.529454  \n",
      "2023-07-20    -0.663450  0.596540  0.370909  \n",
      "2023-07-21     0.000000  0.188168  1.886110  \n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 3.2A STATIC CHRONOLOGICAL SPLIT\n",
    "# ================================================================\n",
    "\n",
    "# Function for chronological splitting of time series data\n",
    "def chronological_split(df, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Chronologically splits the dataset into train, validation, and test sets.\n",
    "    Ensures no look-ahead bias by using the time order of observations.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with DateTime index\n",
    "    - train_ratio: float, proportion of training data\n",
    "    - val_ratio: float, proportion of validation data\n",
    "    - test_ratio: float, proportion of testing data\n",
    "\n",
    "    Returns:\n",
    "    - train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    # Validate ratios\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9, \"Ratios must sum to 1.\"\n",
    "\n",
    "    n = len(df)\n",
    "    i_train_end = int(n * train_ratio)\n",
    "    i_val_end = int(n * (train_ratio + val_ratio))\n",
    "\n",
    "    train_df = df.iloc[:i_train_end].copy()\n",
    "    val_df = df.iloc[i_train_end:i_val_end].copy()\n",
    "    test_df = df.iloc[i_val_end:].copy()\n",
    "\n",
    "    print(f\"Total samples: {n}\")\n",
    "    print(f\"Train: {len(train_df)} ({len(train_df)/n:.1%}), \"\n",
    "          f\"Validation: {len(val_df)} ({len(val_df)/n:.1%}), \"\n",
    "          f\"Test: {len(test_df)} ({len(test_df)/n:.1%})\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Apply chronological split to your processed dataframe\n",
    "# ---------------------------------------------------------------\n",
    "train_df, val_df, test_df = chronological_split(df, 0.8, 0.1, 0.1)\n",
    "\n",
    "# Confirm the split visually\n",
    "print(\"\\nTrain range:\", train_df.index.min(), \"→\", train_df.index.max())\n",
    "print(\"Validation range:\", val_df.index.min(), \"→\", val_df.index.max())\n",
    "print(\"Test range:\", test_df.index.min(), \"→\", test_df.index.max())\n",
    "\n",
    "# Display sample data from each set\n",
    "print(\"\\nSample from training data:\")\n",
    "print(train_df.tail())\n",
    "\n",
    "print(\"\\nSample from validation data:\")\n",
    "print(val_df.head())\n",
    "\n",
    "print(\"\\nSample from test data:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd881e-149c-4077-9cef-6e9e8850eb53",
   "metadata": {},
   "source": [
    "Create functions: AWT and Indicators, scaling and rolling windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6d2bcdfe-b618-493d-a4c4-75551c674d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ✅ FIXED: Walk-Forward Validation with AWT Features + Scaling\n",
    "# ================================================================\n",
    "import pywt\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "# 1️⃣  Utility: Flatten MultiIndex columns\n",
    "# ================================================================\n",
    "def flatten_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure single-level, clean column names.\"\"\"\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\n",
    "            '_'.join([str(c) for c in tup if c is not None]).strip()\n",
    "            for tup in df.columns\n",
    "        ]\n",
    "    else:\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "    # Remove duplicates if flattening causes any\n",
    "    df = df.loc[:, ~pd.Index(df.columns).duplicated()]\n",
    "    return df\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2️⃣  Wavelet Denoising Helper\n",
    "# ================================================================\n",
    "def wavelet_denoise_series(x: pd.Series,\n",
    "                           wavelet: str = \"db2\",\n",
    "                           level: int = 2,\n",
    "                           mode: str = \"soft\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simple wavelet decompose + denoise returning smooth & detail as DataFrame.\n",
    "    \"\"\"\n",
    "    x = pd.Series(x).astype(float)\n",
    "    coeffs = pywt.wavedec(x.values, wavelet=wavelet, level=level)\n",
    "    # Universal threshold based on noise estimation\n",
    "    sigma = np.median(np.abs(coeffs[-1])) / 0.6745 if len(coeffs[-1]) else 0.0\n",
    "    thr = sigma * np.sqrt(2 * np.log(len(x)))\n",
    "    coeffs_thr = [coeffs[0]] + [pywt.threshold(c, thr, mode=mode) for c in coeffs[1:]]\n",
    "    x_s = pywt.waverec(coeffs_thr, wavelet)[:len(x)]  # smoothed component\n",
    "    x_d = x.values[:len(x_s)] - x_s  # detail component\n",
    "    return pd.DataFrame({\"SPY_s\": x_s, \"SPY_d\": x_d}, index=x.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "497ec125-4c14-4fac-8557-4751dd086b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 3️⃣  Compute AWT + Features + Target\n",
    "# ================================================================\n",
    "# ------------------------------------------------\n",
    "# Robust AWT + features + target (auto-make SPY_log_ret if missing)\n",
    "# ------------------------------------------------\n",
    "def _first_present(colnames, df_cols):\n",
    "    for c in colnames:\n",
    "        if c in df_cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def compute_awt_features(df_slice: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build AWT-based features + 21 indicators safely.\n",
    "    Requires SPY OHLCV if available; will fall back where possible.\n",
    "    Produces:\n",
    "      SPY_s, SPY_d, and 21 indicators incl. MACD, RSI, ADX, OBV, MFI, WVAD, etc.\n",
    "    \"\"\"\n",
    "    # -------- helpers --------\n",
    "    def _flatten(df):\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = ['_'.join([str(c) for c in tup if c is not None]).strip()\n",
    "                          for tup in df.columns]\n",
    "        else:\n",
    "            df.columns = [str(c).strip() for c in df.columns]\n",
    "        return df.loc[:, ~pd.Index(df.columns).duplicated()]\n",
    "\n",
    "    def _find_col(df, *must_contain):\n",
    "        must = [m.lower() for m in must_contain]\n",
    "        for c in df.columns:\n",
    "            lc = str(c).lower()\n",
    "            if all(m in lc for m in must):\n",
    "                return c\n",
    "        # simple fallbacks for common single-level names\n",
    "        lut = {\n",
    "            ('spy', 'open'):   ['SPY_Open', 'Open', 'Open_SPY'],\n",
    "            ('spy', 'high'):   ['SPY_High', 'High', 'High_SPY'],\n",
    "            ('spy', 'low'):    ['SPY_Low', 'Low', 'Low_SPY'],\n",
    "            ('spy', 'close'):  ['SPY_Close', 'Close', 'Close_SPY'],\n",
    "            ('spy', 'volume'): ['SPY_Volume', 'Volume', 'Volume_SPY'],\n",
    "        }\n",
    "        for k, alts in lut.items():\n",
    "            if set(k) == set(must):\n",
    "                for a in alts:\n",
    "                    if a in df.columns:\n",
    "                        return a\n",
    "        return None\n",
    "\n",
    "    def _wavelet_denoise(x, wavelet=\"db2\", level=2, mode=\"soft\"):\n",
    "        x = pd.Series(x).astype(float)\n",
    "        coeffs = pywt.wavedec(x.values, wavelet=wavelet, level=level)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745 if len(coeffs[-1]) else 0.0\n",
    "        thr   = sigma * np.sqrt(2 * np.log(len(x)))\n",
    "        coeffs_thr = [coeffs[0]] + [pywt.threshold(c, thr, mode=mode) for c in coeffs[1:]]\n",
    "        x_s = pywt.waverec(coeffs_thr, wavelet)[:len(x)]\n",
    "        x_d = x.values[:len(x_s)] - x_s\n",
    "        return pd.DataFrame({\"SPY_s\": x_s, \"SPY_d\": x_d}, index=x.index)\n",
    "\n",
    "    # -------- start --------\n",
    "    df_f = _flatten(df_slice.copy())\n",
    "\n",
    "    # Locate OHLCV columns (best effort)\n",
    "    c_open   = _find_col(df_f, 'spy', 'open')\n",
    "    c_high   = _find_col(df_f, 'spy', 'high')\n",
    "    c_low    = _find_col(df_f, 'spy', 'low')\n",
    "    c_close  = _find_col(df_f, 'spy', 'close')\n",
    "    c_volume = _find_col(df_f, 'spy', 'volume')\n",
    "\n",
    "    # Ensure log returns exist (needed for AWT)\n",
    "    if 'SPY_log_ret' not in df_f.columns:\n",
    "        if c_close is None:\n",
    "            raise KeyError(\"compute_awt_features() needs SPY close prices to derive SPY_log_ret.\")\n",
    "        df_f['SPY_log_ret'] = np.log(df_f[c_close] / df_f[c_close].shift(1)) * 100\n",
    "\n",
    "    # AWT on SPY_log_ret -> SPY_s (smooth), SPY_d (detail)\n",
    "    s_hat = _wavelet_denoise(df_f['SPY_log_ret'])\n",
    "    df_f = df_f.join(s_hat, how='left')\n",
    "\n",
    "    # Choose price series for TA (prefer denoised)\n",
    "    price = df_f['SPY_s'].fillna(df_f['SPY_log_ret'])\n",
    "\n",
    "    # ---------------- 21 INDICATORS ----------------\n",
    "    # 1–3 Trend MAs\n",
    "    df_f['EMA20'] = ta.trend.EMAIndicator(close=price, window=20).ema_indicator()\n",
    "    df_f['MA5']   = ta.trend.SMAIndicator(close=price, window=5).sma_indicator()\n",
    "    df_f['MA10']  = ta.trend.SMAIndicator(close=price, window=10).sma_indicator()\n",
    "\n",
    "    # 4 MACD\n",
    "    df_f['MACD']  = ta.trend.MACD(close=price).macd()\n",
    "\n",
    "    # 5 ROC(10)\n",
    "    df_f['ROC']   = ta.momentum.ROCIndicator(close=price, window=10).roc()\n",
    "\n",
    "    # 6–7 Momentum (6m, 12m)\n",
    "    df_f['MTM6']  = price / price.shift(126) - 1\n",
    "    df_f['MTM12'] = price / price.shift(252) - 1\n",
    "\n",
    "    # 8 SMI (stochastic momentum index)\n",
    "    if c_high and c_low:\n",
    "        stoch = ta.momentum.StochasticOscillator(high=df_f[c_high], low=df_f[c_low],\n",
    "                                                 close=price, window=14, smooth_window=3)\n",
    "        df_f['SMI'] = stoch.stoch_signal()\n",
    "    else:\n",
    "        df_f['SMI'] = np.nan\n",
    "\n",
    "    # 9 ATR(14)\n",
    "    df_f['ATR'] = (ta.volatility.AverageTrueRange(\n",
    "        high=df_f[c_high] if c_high else price,\n",
    "        low=df_f[c_low] if c_low else price,\n",
    "        close=price, window=14\n",
    "    ).average_true_range())\n",
    "\n",
    "    # 10–12 Bollinger\n",
    "    boll = ta.volatility.BollingerBands(close=price, window=20, window_dev=2)\n",
    "    df_f['BOLL_upper']     = boll.bollinger_hband()\n",
    "    df_f['BOLL_lower']     = boll.bollinger_lband()\n",
    "    df_f['BOLL_bandwidth'] = df_f['BOLL_upper'] - df_f['BOLL_lower']\n",
    "\n",
    "    # 13 CCI(20)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['CCI'] = ta.trend.CCIIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], window=20\n",
    "        ).cci()\n",
    "    else:\n",
    "        df_f['CCI'] = np.nan\n",
    "\n",
    "    # 14 WVAD (needs OHLCV)\n",
    "    if c_open and c_high and c_low and c_close and c_volume:\n",
    "        rng = (df_f[c_high] - df_f[c_low]).replace(0, np.nan)\n",
    "        df_f['WVAD'] = ((df_f[c_close] - df_f[c_open]) / (rng + 1e-10)) * df_f[c_volume]\n",
    "    else:\n",
    "        df_f['WVAD'] = np.nan\n",
    "\n",
    "    # 15 RSI(14)\n",
    "    df_f['RSI14'] = ta.momentum.RSIIndicator(close=price, window=14).rsi()\n",
    "\n",
    "    # 16–17 Stochastic %K/%D (classic)\n",
    "    if c_high and c_low:\n",
    "        st = ta.momentum.StochasticOscillator(high=df_f[c_high], low=df_f[c_low],\n",
    "                                              close=price, window=14, smooth_window=3)\n",
    "        df_f['STO_K'] = st.stoch()\n",
    "        df_f['STO_D'] = st.stoch_signal()\n",
    "    else:\n",
    "        df_f[['STO_K', 'STO_D']] = np.nan\n",
    "\n",
    "    # 18 ADX(14)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['ADX14'] = ta.trend.ADXIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], window=14\n",
    "        ).adx()\n",
    "    else:\n",
    "        df_f['ADX14'] = np.nan\n",
    "\n",
    "    # 19 OBV\n",
    "    if c_close and c_volume:\n",
    "        df_f['OBV'] = ta.volume.OnBalanceVolumeIndicator(\n",
    "            close=df_f[c_close], volume=df_f[c_volume]\n",
    "        ).on_balance_volume()\n",
    "    else:\n",
    "        df_f['OBV'] = np.nan\n",
    "\n",
    "    # 20 MFI(14)\n",
    "    if c_high and c_low and c_close and c_volume:\n",
    "        df_f['MFI14'] = ta.volume.MFIIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low],\n",
    "            close=df_f[c_close], volume=df_f[c_volume], window=14\n",
    "        ).money_flow_index()\n",
    "    else:\n",
    "        df_f['MFI14'] = np.nan\n",
    "\n",
    "    # 21 Williams %R(14)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['WILLR14'] = ta.momentum.WilliamsRIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], lbp=14\n",
    "        ).williams_r()\n",
    "    else:\n",
    "        df_f['WILLR14'] = np.nan\n",
    "\n",
    "    # Target (example: next-day log return)\n",
    "    df_f['y_1d'] = df_f['SPY_log_ret'].shift(-1)\n",
    "\n",
    "    # Final clean (drop rows introduced by windows/shifts)\n",
    "    return df_f.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "45001d14-12c3-4da2-ab28-8728556a7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 4️⃣  Scaling Helper\n",
    "# ================================================================\n",
    "def fit_transform_scale(X_train, X_val=None, X_test=None):\n",
    "    \"\"\"\n",
    "    Fit StandardScaler on training set; transform others with same parameters.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(X_train)\n",
    "    Xva = scaler.transform(X_val) if X_val is not None else None\n",
    "    Xte = scaler.transform(X_test) if X_test is not None else None\n",
    "    return Xtr, Xva, Xte, scaler\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5️⃣  Rolling Window Generator\n",
    "# ================================================================\n",
    "def walk_forward_windows(df, train_span, eval_span, step):\n",
    "    \"\"\"\n",
    "    Generator producing chronological (train_df, eval_df) pairs.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    start = 0\n",
    "    while True:\n",
    "        tr_start = start\n",
    "        tr_end   = tr_start + train_span\n",
    "        ev_end   = tr_end + eval_span\n",
    "        if ev_end > n:\n",
    "            break\n",
    "        yield df.iloc[tr_start:tr_end].copy(), df.iloc[tr_end:ev_end].copy()\n",
    "        start += step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "95291d6b-f0a9-4092-afcb-67432c65839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Walk-forward loop ran with NaN-safe slicing and window guards.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 6️⃣  Walk-Forward Execution Loop\n",
    "# ================================================================\n",
    "# --- REPLACE your walk-forward loop with this safer version ---\n",
    "TRAIN_SPAN = 1500      # ~6 years\n",
    "EVAL_SPAN  = 60        # ~3 months\n",
    "STEP       = 20        # ~1 month shift\n",
    "\n",
    "feature_cols = None\n",
    "target_col   = 'y_1d'\n",
    "\n",
    "MIN_TR_ROWS  = 100   # guard: minimum rows required after dropna\n",
    "MIN_EV_ROWS  = 10\n",
    "\n",
    "all_preds, all_truth = [], []\n",
    "\n",
    "made_plots = False  # Track if we’ve already plotted\n",
    "\n",
    "for w, (df_tr_raw, df_ev_raw) in enumerate(walk_forward_windows(df, TRAIN_SPAN, EVAL_SPAN, STEP), start=1):\n",
    "    df_tr_raw = flatten_cols(df_tr_raw)\n",
    "    df_ev_raw = flatten_cols(df_ev_raw)\n",
    "\n",
    "    # Build features on combined slice\n",
    "    df_comb       = pd.concat([df_tr_raw, df_ev_raw], axis=0)\n",
    "    df_feat_comb  = compute_awt_features(df_comb)\n",
    "\n",
    "    # Split back to train/eval by index, THEN drop NaNs\n",
    "    tr_end_idx = df_tr_raw.index[-1]\n",
    "    df_feat_tr = df_feat_comb.loc[:tr_end_idx].dropna()\n",
    "    df_feat_ev = df_feat_comb.loc[df_ev_raw.index[0]:df_ev_raw.index[-1]].dropna()\n",
    "\n",
    "    # Window too small? (happens in first windows due to long lookbacks)\n",
    "    if len(df_feat_tr) < MIN_TR_ROWS or len(df_feat_ev) < MIN_EV_ROWS:\n",
    "        # optional: print(f\"Skipping window {w}: train={len(df_feat_tr)}, eval={len(df_feat_ev)}\")\n",
    "        continue\n",
    "\n",
    "    # Choose features once (exclude raw returns & targets)\n",
    "    if feature_cols is None:\n",
    "        exclude = {'SPY_log_ret', 'DXY_ret', 'WTI_ret'} | {c for c in df_feat_tr.columns if c.startswith('y_')}\n",
    "        feature_cols = [c for c in df_feat_tr.columns if c not in exclude]\n",
    "\n",
    "    X_tr = df_feat_tr[feature_cols].values\n",
    "    y_tr = df_feat_tr[target_col].values\n",
    "    X_ev = df_feat_ev[feature_cols].values\n",
    "    y_ev = df_feat_ev[target_col].values\n",
    "\n",
    "    # Another guard (just in case)\n",
    "    if X_tr.shape[0] == 0 or X_ev.shape[0] == 0:\n",
    "        # optional: print(f\"Skipping window {w}: empty matrix after selection\")\n",
    "        continue\n",
    "\n",
    "    # Scale using train stats only\n",
    "    X_tr_s, _, X_ev_s, scaler = fit_transform_scale(X_tr, None, X_ev)\n",
    "\n",
    "    # Fit & predict here when ready:\n",
    "    # model.fit(X_tr_s, y_tr)\n",
    "    # y_hat = model.predict(X_ev_s)\n",
    "    # all_preds.append(y_hat); all_truth.append(y_ev)\n",
    "\n",
    "print(\"✅ Walk-forward loop ran with NaN-safe slicing and window guards.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LSTM_Research_Project)",
   "language": "python",
   "name": "lstm_research_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
