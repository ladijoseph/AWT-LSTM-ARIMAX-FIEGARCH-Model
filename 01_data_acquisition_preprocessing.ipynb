{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df2ba3b-890b-4272-9dce-9ba8bd9006ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo Finance Data Acquisition of Required Data\n",
    "#Target Period: 2006-01-01 to 2025-06-30\n",
    "\n",
    "#Imports\n",
    "#Import Yahoo Finance data library for data acquisition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120b0669-fbe8-4962-ba2b-5e47173eb392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_76596\\3065575225.py:6: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df_spx_raw = yf.download(\"SPY\", start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_76596\\3065575225.py:9: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df_dxy = yf.download(\"DX-Y.NYB\", start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_76596\\3065575225.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df_wti = yf.download(\"CL=F\", start=start, end=end)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price           Close       High        Low       Open    Volume\n",
      "Ticker            SPY        SPY        SPY        SPY       SPY\n",
      "Date                                                            \n",
      "2006-01-03  87.721687  87.929397  86.122343  86.676231  73256700\n",
      "2006-01-04  88.137154  88.268698  87.721735  87.832514  51899600\n",
      "2006-01-05  88.192543  88.337938  87.846364  88.033304  47307500\n",
      "2006-01-06  88.926407  89.023336  88.178659  88.635618  62885900\n",
      "2006-01-09  89.154869  89.355648  88.884850  88.912540  43527400\n",
      "Price           Close       High        Low       Open   Volume\n",
      "Ticker       DX-Y.NYB   DX-Y.NYB   DX-Y.NYB   DX-Y.NYB DX-Y.NYB\n",
      "Date                                                           \n",
      "2006-01-03  89.839996  90.940002  89.779999  90.750000        0\n",
      "2006-01-04  89.139999  89.860001  89.010002  89.730003        0\n",
      "2006-01-05  89.330002  89.629997  89.180000  89.269997        0\n",
      "2006-01-06  88.849998  89.629997  88.800003  89.349998        0\n",
      "2006-01-09  89.250000  89.449997  88.809998  88.959999        0\n",
      "Price           Close       High        Low       Open  Volume\n",
      "Ticker           CL=F       CL=F       CL=F       CL=F    CL=F\n",
      "Date                                                          \n",
      "2006-01-03  63.139999  63.799999  60.810001  61.040001  130635\n",
      "2006-01-04  63.419998  63.650002  62.259998  63.000000  105194\n",
      "2006-01-05  62.790001  63.799999  62.599998  63.400002  104035\n",
      "2006-01-06  64.209999  64.449997  62.599998  62.599998  110763\n",
      "2006-01-09  63.500000  64.610001  62.900002  64.150002  115558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Define instruments and Date range\n",
    "start = \"2006-01-01\"\n",
    "end = \"2025-06-30\"\n",
    "\n",
    "# S&P500 ETF (proxy)\n",
    "df_spx_raw = yf.download(\"SPY\", start=start, end=end)\n",
    "\n",
    "# Dollar Index (Yahoo ticker: DX-Y.NYB)\n",
    "df_dxy = yf.download(\"DX-Y.NYB\", start=start, end=end)\n",
    "\n",
    "# WTI Oil Futures (Yahoo ticker: CL=F)\n",
    "df_wti = yf.download(\"CL=F\", start=start, end=end)\n",
    "\n",
    "print(df_spx_raw.head())\n",
    "print(df_dxy.head())\n",
    "print(df_wti.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81563e-688b-46b7-b0c7-8a2382a8bf60",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e88553b-165d-426d-a153-e748dbc39e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY shape: (4902, 5)\n",
      "DXY shape: (4905, 5)\n",
      "WTI shape: (4901, 5)\n",
      "Merged dataframe shape: (4898, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\anaconda3\\envs\\LSTM_Research_Project\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:395: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#Load and confirm the raw datasets\n",
    "print(\"SPY shape:\", df_spx_raw.shape)\n",
    "print(\"DXY shape:\", df_dxy.shape)\n",
    "print(\"WTI shape:\", df_wti.shape)\n",
    "\n",
    "#Standardise column names and isolate 'Close' column\n",
    "df_spx = df_spx_raw[['Close']].rename(columns={'Close': 'SPY_Close'})\n",
    "df_dxy = df_dxy[['Close']].rename(columns={'Close': 'DXY_Close'})\n",
    "df_wti = df_wti[['Close']].rename(columns={'Close': 'WTI_Close'})\n",
    "\n",
    "#Merge and clean datasets\n",
    "df = df_spx.join([df_dxy, df_wti], how='inner')\n",
    "df = df.dropna()\n",
    "print(\"Merged dataframe shape:\", df.shape)\n",
    "\n",
    "#Compute returns and changes\n",
    "df['SPY_log_ret'] = np.log(df['SPY_Close'] / df['SPY_Close'].shift(1)) * 100\n",
    "df['SPY_price_change'] = df['SPY_Close'].diff()\n",
    "df['SPY_p_change'] = df['SPY_Close'].pct_change() * 100\n",
    "\n",
    "df['DXY_ret'] = np.log(df['DXY_Close'] / df['DXY_Close'].shift(1)) * 100\n",
    "df['WTI_ret'] = np.log(df['WTI_Close'] / df['WTI_Close'].shift(1)) * 100\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5c77d-0044-46bf-bc47-41987d2b4092",
   "metadata": {},
   "source": [
    "# Time Series Splitting & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8fa31-192d-4d9b-99e0-035f32e81264",
   "metadata": {},
   "source": [
    "# Static Chronological Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cafe036-c812-4db8-89ce-663351d2e7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 4895\n",
      "Train: 3916 (80.0%), Validation: 489 (10.0%), Test: 490 (10.0%)\n",
      "\n",
      "Train range: 2006-01-04 00:00:00 → 2021-08-03 00:00:00\n",
      "Validation range: 2021-08-04 00:00:00 → 2023-07-14 00:00:00\n",
      "Test range: 2023-07-17 00:00:00 → 2025-06-27 00:00:00\n",
      "\n",
      "Sample from training data:\n",
      "Price        SPY_Close  DXY_Close  WTI_Close SPY_log_ret SPY_price_change  \\\n",
      "Ticker             SPY   DX-Y.NYB       CL=F                                \n",
      "Date                                                                        \n",
      "2021-07-28  413.542023  92.320000  72.389999   -0.041015        -0.169647   \n",
      "2021-07-29  415.257111  91.860001  73.620003    0.413874         1.715088   \n",
      "2021-07-30  413.240509  92.169998  73.949997   -0.486810        -2.016602   \n",
      "2021-08-02  412.373535  92.050003  71.260002   -0.210019        -0.866974   \n",
      "2021-08-03  415.728302  92.080002  70.559998    0.810235         3.354767   \n",
      "\n",
      "Price      SPY_p_change   DXY_ret   WTI_ret  \n",
      "Ticker                                       \n",
      "Date                                         \n",
      "2021-07-28    -0.041006 -0.119081  1.027498  \n",
      "2021-07-29     0.414731 -0.499511  1.684861  \n",
      "2021-07-30    -0.485627  0.336899  0.447238  \n",
      "2021-08-02    -0.209799 -0.130274 -3.705396  \n",
      "2021-08-03     0.813526  0.032584 -0.987181  \n",
      "\n",
      "Sample from validation data:\n",
      "Price        SPY_Close  DXY_Close  WTI_Close SPY_log_ret SPY_price_change  \\\n",
      "Ticker             SPY   DX-Y.NYB       CL=F                                \n",
      "Date                                                                        \n",
      "2021-08-04  413.683411  92.269997  68.150002   -0.493095        -2.044891   \n",
      "2021-08-05  416.303223  92.239998  69.089996    0.631292         2.619812   \n",
      "2021-08-06  416.991089  92.800003  68.279999    0.165096         0.687866   \n",
      "2021-08-09  416.651855  92.949997  66.480003   -0.081386        -0.339233   \n",
      "2021-08-10  417.170227  93.059998  68.290001    0.124336         0.518372   \n",
      "\n",
      "Price      SPY_p_change   DXY_ret   WTI_ret  \n",
      "Ticker                                       \n",
      "Date                                         \n",
      "2021-08-04    -0.491882  0.206124 -3.475220  \n",
      "2021-08-05     0.633289 -0.032517  1.369877  \n",
      "2021-08-06     0.165232  0.605282 -1.179307  \n",
      "2021-08-09    -0.081353  0.161501 -2.671568  \n",
      "2021-08-10     0.124414  0.118274  2.686216  \n",
      "\n",
      "Sample from test data:\n",
      "Price        SPY_Close   DXY_Close  WTI_Close SPY_log_ret SPY_price_change  \\\n",
      "Ticker             SPY    DX-Y.NYB       CL=F                                \n",
      "Date                                                                         \n",
      "2023-07-17  437.937042   99.839996  74.150002    0.346599         1.515259   \n",
      "2023-07-18  441.191223   99.940002  75.750000    0.740323         3.254181   \n",
      "2023-07-19  442.172302  100.279999  75.349998    0.222124         0.981079   \n",
      "2023-07-20  439.238739  100.879997  75.629997   -0.665654        -2.933563   \n",
      "2023-07-21  439.238739  101.070000  77.070000    0.000000         0.000000   \n",
      "\n",
      "Price      SPY_p_change   DXY_ret   WTI_ret  \n",
      "Ticker                                       \n",
      "Date                                         \n",
      "2023-07-17     0.347201 -0.070095 -1.698238  \n",
      "2023-07-18     0.743070  0.100116  2.134836  \n",
      "2023-07-19     0.222370  0.339623 -0.529454  \n",
      "2023-07-20    -0.663443  0.596540  0.370909  \n",
      "2023-07-21     0.000000  0.188168  1.886110  \n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 3.2A STATIC CHRONOLOGICAL SPLIT\n",
    "# ================================================================\n",
    "\n",
    "# Function for chronological splitting of time series data\n",
    "def chronological_split(df, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Chronologically splits the dataset into train, validation, and test sets.\n",
    "    Ensures no look-ahead bias by using the time order of observations.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with DateTime index\n",
    "    - train_ratio: float, proportion of training data\n",
    "    - val_ratio: float, proportion of validation data\n",
    "    - test_ratio: float, proportion of testing data\n",
    "\n",
    "    Returns:\n",
    "    - train_df, val_df, test_df\n",
    "    \"\"\"\n",
    "    # Validate ratios\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9, \"Ratios must sum to 1.\"\n",
    "\n",
    "    n = len(df)\n",
    "    i_train_end = int(n * train_ratio)\n",
    "    i_val_end = int(n * (train_ratio + val_ratio))\n",
    "\n",
    "    train_df = df.iloc[:i_train_end].copy()\n",
    "    val_df = df.iloc[i_train_end:i_val_end].copy()\n",
    "    test_df = df.iloc[i_val_end:].copy()\n",
    "\n",
    "    print(f\"Total samples: {n}\")\n",
    "    print(f\"Train: {len(train_df)} ({len(train_df)/n:.1%}), \"\n",
    "          f\"Validation: {len(val_df)} ({len(val_df)/n:.1%}), \"\n",
    "          f\"Test: {len(test_df)} ({len(test_df)/n:.1%})\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Apply chronological split to your processed dataframe\n",
    "# ---------------------------------------------------------------\n",
    "train_df, val_df, test_df = chronological_split(df, 0.8, 0.1, 0.1)\n",
    "\n",
    "# Confirm the split visually\n",
    "print(\"\\nTrain range:\", train_df.index.min(), \"→\", train_df.index.max())\n",
    "print(\"Validation range:\", val_df.index.min(), \"→\", val_df.index.max())\n",
    "print(\"Test range:\", test_df.index.min(), \"→\", test_df.index.max())\n",
    "\n",
    "# Display sample data from each set\n",
    "print(\"\\nSample from training data:\")\n",
    "print(train_df.tail())\n",
    "\n",
    "print(\"\\nSample from validation data:\")\n",
    "print(val_df.head())\n",
    "\n",
    "print(\"\\nSample from test data:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd881e-149c-4077-9cef-6e9e8850eb53",
   "metadata": {},
   "source": [
    "Create functions: AWT and Indicators, scaling and rolling windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e5ad3-2f25-4136-8fab-0592d37dfe44",
   "metadata": {},
   "source": [
    "# Rolling Window (Walk-Forward Validation) and Adaptive Wavelength Transform (AWT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1dbe1-4278-4e8e-9862-ee40761f8195",
   "metadata": {},
   "source": [
    "# Flatten multi-index columns and apply wavelet denoising helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d2bcdfe-b618-493d-a4c4-75551c674d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ✅ FIXED: Walk-Forward Validation with AWT Features + Scaling\n",
    "# ================================================================\n",
    "import pywt\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ================================================================\n",
    "# 1️⃣  Utility: Flatten MultiIndex columns\n",
    "# ================================================================\n",
    "def flatten_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure single-level, clean column names.\"\"\"\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\n",
    "            '_'.join([str(c) for c in tup if c is not None]).strip()\n",
    "            for tup in df.columns\n",
    "        ]\n",
    "    else:\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "    # Remove duplicates if flattening causes any\n",
    "    df = df.loc[:, ~pd.Index(df.columns).duplicated()]\n",
    "    return df\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2️⃣  Wavelet Denoising Helper\n",
    "# ================================================================\n",
    "def wavelet_denoise_series(x: pd.Series,\n",
    "                           wavelet: str = \"db2\",\n",
    "                           level: int = 2,\n",
    "                           mode: str = \"soft\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simple wavelet decompose + denoise returning smooth & detail as DataFrame.\n",
    "    \"\"\"\n",
    "    x = pd.Series(x).astype(float)\n",
    "    coeffs = pywt.wavedec(x.values, wavelet=wavelet, level=level)\n",
    "    # Universal threshold based on noise estimation\n",
    "    sigma = np.median(np.abs(coeffs[-1])) / 0.6745 if len(coeffs[-1]) else 0.0\n",
    "    thr = sigma * np.sqrt(2 * np.log(len(x)))\n",
    "    coeffs_thr = [coeffs[0]] + [pywt.threshold(c, thr, mode=mode) for c in coeffs[1:]]\n",
    "    x_s = pywt.waverec(coeffs_thr, wavelet)[:len(x)]  # smoothed component\n",
    "    x_d = x.values[:len(x_s)] - x_s  # detail component\n",
    "    return pd.DataFrame({\"SPY_s\": x_s, \"SPY_d\": x_d}, index=x.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d263b27-5165-4f1f-af6f-5c526c90db66",
   "metadata": {},
   "source": [
    "# Build AWT features and 21 financial indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "497ec125-4c14-4fac-8557-4751dd086b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 3️⃣  Compute AWT + Features + Target\n",
    "# ================================================================\n",
    "# ------------------------------------------------\n",
    "# Robust AWT + features + target (auto-make SPY_log_ret if missing)\n",
    "# ------------------------------------------------\n",
    "def _first_present(colnames, df_cols):\n",
    "    for c in colnames:\n",
    "        if c in df_cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def compute_awt_features(df_slice: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build AWT-based features + 21 indicators safely.\n",
    "    Requires SPY OHLCV if available; will fall back where possible.\n",
    "    Produces:\n",
    "      SPY_s, SPY_d, and 21 indicators incl. MACD, RSI, ADX, OBV, MFI, WVAD, etc.\n",
    "    \"\"\"\n",
    "    # -------- helpers --------\n",
    "    def _flatten(df):\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = ['_'.join([str(c) for c in tup if c is not None]).strip()\n",
    "                          for tup in df.columns]\n",
    "        else:\n",
    "            df.columns = [str(c).strip() for c in df.columns]\n",
    "        return df.loc[:, ~pd.Index(df.columns).duplicated()]\n",
    "\n",
    "    def _find_col(df, *must_contain):\n",
    "        must = [m.lower() for m in must_contain]\n",
    "        for c in df.columns:\n",
    "            lc = str(c).lower()\n",
    "            if all(m in lc for m in must):\n",
    "                return c\n",
    "        # simple fallbacks for common single-level names\n",
    "        lut = {\n",
    "            ('spy', 'open'):   ['SPY_Open', 'Open', 'Open_SPY'],\n",
    "            ('spy', 'high'):   ['SPY_High', 'High', 'High_SPY'],\n",
    "            ('spy', 'low'):    ['SPY_Low', 'Low', 'Low_SPY'],\n",
    "            ('spy', 'close'):  ['SPY_Close', 'Close', 'Close_SPY'],\n",
    "            ('spy', 'volume'): ['SPY_Volume', 'Volume', 'Volume_SPY'],\n",
    "        }\n",
    "        for k, alts in lut.items():\n",
    "            if set(k) == set(must):\n",
    "                for a in alts:\n",
    "                    if a in df.columns:\n",
    "                        return a\n",
    "        return None\n",
    "\n",
    "    def _wavelet_denoise(x, wavelet=\"db2\", level=2, mode=\"soft\"):\n",
    "        x = pd.Series(x).astype(float)\n",
    "        coeffs = pywt.wavedec(x.values, wavelet=wavelet, level=level)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745 if len(coeffs[-1]) else 0.0\n",
    "        thr   = sigma * np.sqrt(2 * np.log(len(x)))\n",
    "        coeffs_thr = [coeffs[0]] + [pywt.threshold(c, thr, mode=mode) for c in coeffs[1:]]\n",
    "        x_s = pywt.waverec(coeffs_thr, wavelet)[:len(x)]\n",
    "        x_d = x.values[:len(x_s)] - x_s\n",
    "        return pd.DataFrame({\"SPY_s\": x_s, \"SPY_d\": x_d}, index=x.index)\n",
    "\n",
    "    # -------- start --------\n",
    "    df_f = _flatten(df_slice.copy())\n",
    "\n",
    "    # Locate OHLCV columns (best effort)\n",
    "    c_open   = _find_col(df_f, 'spy', 'open')\n",
    "    c_high   = _find_col(df_f, 'spy', 'high')\n",
    "    c_low    = _find_col(df_f, 'spy', 'low')\n",
    "    c_close  = _find_col(df_f, 'spy', 'close')\n",
    "    c_volume = _find_col(df_f, 'spy', 'volume')\n",
    "\n",
    "    # Ensure log returns exist (needed for AWT)\n",
    "    if 'SPY_log_ret' not in df_f.columns:\n",
    "        if c_close is None:\n",
    "            raise KeyError(\"compute_awt_features() needs SPY close prices to derive SPY_log_ret.\")\n",
    "        df_f['SPY_log_ret'] = np.log(df_f[c_close] / df_f[c_close].shift(1)) * 100\n",
    "\n",
    "    # AWT on SPY_log_ret -> SPY_s (smooth), SPY_d (detail)\n",
    "    s_hat = _wavelet_denoise(df_f['SPY_log_ret'])\n",
    "    df_f = df_f.join(s_hat, how='left')\n",
    "\n",
    "    # Choose price series for TA (prefer denoised)\n",
    "    price = df_f['SPY_s'].fillna(df_f['SPY_log_ret'])\n",
    "\n",
    "    # ---------------- 21 INDICATORS ----------------\n",
    "    # 1–3 Trend MAs\n",
    "    df_f['EMA20'] = ta.trend.EMAIndicator(close=price, window=20).ema_indicator()\n",
    "    df_f['MA5']   = ta.trend.SMAIndicator(close=price, window=5).sma_indicator()\n",
    "    df_f['MA10']  = ta.trend.SMAIndicator(close=price, window=10).sma_indicator()\n",
    "\n",
    "    # 4 MACD\n",
    "    df_f['MACD']  = ta.trend.MACD(close=price).macd()\n",
    "\n",
    "    # 5 ROC(10)\n",
    "    df_f['ROC']   = ta.momentum.ROCIndicator(close=price, window=10).roc()\n",
    "\n",
    "    # 6–7 Momentum (6m, 12m)\n",
    "    df_f['MTM6']  = price / price.shift(126) - 1\n",
    "    df_f['MTM12'] = price / price.shift(252) - 1\n",
    "\n",
    "    # 8 SMI (stochastic momentum index)\n",
    "    if c_high and c_low:\n",
    "        stoch = ta.momentum.StochasticOscillator(high=df_f[c_high], low=df_f[c_low],\n",
    "                                                 close=price, window=14, smooth_window=3)\n",
    "        df_f['SMI'] = stoch.stoch_signal()\n",
    "    else:\n",
    "        df_f['SMI'] = np.nan\n",
    "\n",
    "    # 9 ATR(14)\n",
    "    df_f['ATR'] = (ta.volatility.AverageTrueRange(\n",
    "        high=df_f[c_high] if c_high else price,\n",
    "        low=df_f[c_low] if c_low else price,\n",
    "        close=price, window=14\n",
    "    ).average_true_range())\n",
    "\n",
    "    # 10–12 Bollinger\n",
    "    boll = ta.volatility.BollingerBands(close=price, window=20, window_dev=2)\n",
    "    df_f['BOLL_upper']     = boll.bollinger_hband()\n",
    "    df_f['BOLL_lower']     = boll.bollinger_lband()\n",
    "    df_f['BOLL_bandwidth'] = df_f['BOLL_upper'] - df_f['BOLL_lower']\n",
    "\n",
    "    # 13 CCI(20)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['CCI'] = ta.trend.CCIIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], window=20\n",
    "        ).cci()\n",
    "    else:\n",
    "        df_f['CCI'] = np.nan\n",
    "\n",
    "    # 14 WVAD (needs OHLCV)\n",
    "    if c_open and c_high and c_low and c_close and c_volume:\n",
    "        rng = (df_f[c_high] - df_f[c_low]).replace(0, np.nan)\n",
    "        df_f['WVAD'] = ((df_f[c_close] - df_f[c_open]) / (rng + 1e-10)) * df_f[c_volume]\n",
    "    else:\n",
    "        df_f['WVAD'] = np.nan\n",
    "\n",
    "    # 15 RSI(14)\n",
    "    df_f['RSI14'] = ta.momentum.RSIIndicator(close=price, window=14).rsi()\n",
    "\n",
    "    # 16–17 Stochastic %K/%D (classic)\n",
    "    if c_high and c_low:\n",
    "        st = ta.momentum.StochasticOscillator(high=df_f[c_high], low=df_f[c_low],\n",
    "                                              close=price, window=14, smooth_window=3)\n",
    "        df_f['STO_K'] = st.stoch()\n",
    "        df_f['STO_D'] = st.stoch_signal()\n",
    "    else:\n",
    "        df_f[['STO_K', 'STO_D']] = np.nan\n",
    "\n",
    "    # 18 ADX(14)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['ADX14'] = ta.trend.ADXIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], window=14\n",
    "        ).adx()\n",
    "    else:\n",
    "        df_f['ADX14'] = np.nan\n",
    "\n",
    "    # 19 OBV\n",
    "    if c_close and c_volume:\n",
    "        df_f['OBV'] = ta.volume.OnBalanceVolumeIndicator(\n",
    "            close=df_f[c_close], volume=df_f[c_volume]\n",
    "        ).on_balance_volume()\n",
    "    else:\n",
    "        df_f['OBV'] = np.nan\n",
    "\n",
    "    # 20 MFI(14)\n",
    "    if c_high and c_low and c_close and c_volume:\n",
    "        df_f['MFI14'] = ta.volume.MFIIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low],\n",
    "            close=df_f[c_close], volume=df_f[c_volume], window=14\n",
    "        ).money_flow_index()\n",
    "    else:\n",
    "        df_f['MFI14'] = np.nan\n",
    "\n",
    "    # 21 Williams %R(14)\n",
    "    if c_high and c_low and c_close:\n",
    "        df_f['WILLR14'] = ta.momentum.WilliamsRIndicator(\n",
    "            high=df_f[c_high], low=df_f[c_low], close=df_f[c_close], lbp=14\n",
    "        ).williams_r()\n",
    "    else:\n",
    "        df_f['WILLR14'] = np.nan\n",
    "\n",
    "    # Target (example: next-day log return)\n",
    "    df_f['y_1d'] = df_f['SPY_log_ret'].shift(-1)\n",
    "\n",
    "    # Final clean (drop rows introduced by windows/shifts)\n",
    "    return df_f.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355d6c1-1bc2-43ce-9f5b-ab76d8206c2f",
   "metadata": {},
   "source": [
    "# Add Scaling Helper and Rolling Window Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45001d14-12c3-4da2-ab28-8728556a7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 4️⃣  Scaling Helper\n",
    "# ================================================================\n",
    "def fit_transform_scale(X_train, X_val=None, X_test=None):\n",
    "    \"\"\"\n",
    "    Fit StandardScaler on training set; transform others with same parameters.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = scaler.fit_transform(X_train)\n",
    "    Xva = scaler.transform(X_val) if X_val is not None else None\n",
    "    Xte = scaler.transform(X_test) if X_test is not None else None\n",
    "    return Xtr, Xva, Xte, scaler\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5️⃣  Rolling Window Generator\n",
    "# ================================================================\n",
    "def walk_forward_windows(df, train_span, eval_span, step):\n",
    "    \"\"\"\n",
    "    Generator producing chronological (train_df, eval_df) pairs.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    start = 0\n",
    "    while True:\n",
    "        tr_start = start\n",
    "        tr_end   = tr_start + train_span\n",
    "        ev_end   = tr_end + eval_span\n",
    "        if ev_end > n:\n",
    "            break\n",
    "        yield df.iloc[tr_start:tr_end].copy(), df.iloc[tr_end:ev_end].copy()\n",
    "        start += step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac913ba-07f4-4bad-ae52-689cf2fa54a8",
   "metadata": {},
   "source": [
    "# Implement walk forward validation using pre-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95291d6b-f0a9-4092-afcb-67432c65839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Walk-forward loop ran with NaN-safe slicing and window guards.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 6️⃣  Walk-Forward Execution Loop\n",
    "# ================================================================\n",
    "# --- REPLACE your walk-forward loop with this safer version ---\n",
    "TRAIN_SPAN = 1500      # ~6 years\n",
    "EVAL_SPAN  = 60        # ~3 months\n",
    "STEP       = 20        # ~1 month shift\n",
    "\n",
    "feature_cols = None\n",
    "target_col   = 'y_1d'\n",
    "\n",
    "MIN_TR_ROWS  = 100   # guard: minimum rows required after dropna\n",
    "MIN_EV_ROWS  = 10\n",
    "\n",
    "all_preds, all_truth = [], []\n",
    "\n",
    "made_plots = False  # Track if we’ve already plotted\n",
    "\n",
    "for w, (df_tr_raw, df_ev_raw) in enumerate(walk_forward_windows(df, TRAIN_SPAN, EVAL_SPAN, STEP), start=1):\n",
    "    df_tr_raw = flatten_cols(df_tr_raw)\n",
    "    df_ev_raw = flatten_cols(df_ev_raw)\n",
    "\n",
    "    # Build features on combined slice\n",
    "    df_comb       = pd.concat([df_tr_raw, df_ev_raw], axis=0)\n",
    "    df_feat_comb  = compute_awt_features(df_comb)\n",
    "\n",
    "    # Split back to train/eval by index, THEN drop NaNs\n",
    "    tr_end_idx = df_tr_raw.index[-1]\n",
    "    df_feat_tr = df_feat_comb.loc[:tr_end_idx].dropna()\n",
    "    df_feat_ev = df_feat_comb.loc[df_ev_raw.index[0]:df_ev_raw.index[-1]].dropna()\n",
    "\n",
    "    # Window too small? (happens in first windows due to long lookbacks)\n",
    "    if len(df_feat_tr) < MIN_TR_ROWS or len(df_feat_ev) < MIN_EV_ROWS:\n",
    "        # optional: print(f\"Skipping window {w}: train={len(df_feat_tr)}, eval={len(df_feat_ev)}\")\n",
    "        continue\n",
    "\n",
    "    # Choose features once (exclude raw returns & targets)\n",
    "    if feature_cols is None:\n",
    "        exclude = {'SPY_log_ret', 'DXY_ret', 'WTI_ret'} | {c for c in df_feat_tr.columns if c.startswith('y_')}\n",
    "        feature_cols = [c for c in df_feat_tr.columns if c not in exclude]\n",
    "\n",
    "    X_tr = df_feat_tr[feature_cols].values\n",
    "    y_tr = df_feat_tr[target_col].values\n",
    "    X_ev = df_feat_ev[feature_cols].values\n",
    "    y_ev = df_feat_ev[target_col].values\n",
    "\n",
    "    # Another guard (just in case)\n",
    "    if X_tr.shape[0] == 0 or X_ev.shape[0] == 0:\n",
    "        # optional: print(f\"Skipping window {w}: empty matrix after selection\")\n",
    "        continue\n",
    "\n",
    "    # Scale using train stats only\n",
    "    X_tr_s, _, X_ev_s, scaler = fit_transform_scale(X_tr, None, X_ev)\n",
    "\n",
    "    # Fit & predict here when ready:\n",
    "    # model.fit(X_tr_s, y_tr)\n",
    "    # y_hat = model.predict(X_ev_s)\n",
    "    # all_preds.append(y_hat); all_truth.append(y_ev)\n",
    "\n",
    "print(\"✅ Walk-forward loop ran with NaN-safe slicing and window guards.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4560e8-ed07-4d20-bc37-2c8d96f42038",
   "metadata": {},
   "source": [
    "# Stationarity Tests (ADF & PP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dda997-1965-42c2-a9fd-d41be17a74cd",
   "metadata": {},
   "source": [
    "Stationarity: If ADF/PP p-values < 0.05, treat SPY_log_ret as stationary (set ARIMAX d=0).\n",
    "\n",
    "Long memory:\n",
    "\n",
    "R/S (Hurst H): H≈0.5 → short memory; H>0.5 → persistent/long memory; H<0.5 → anti-persistence.\n",
    "\n",
    "GPH d: 0 ≤ d < 0.5 indicates long memory but stationary, motivating ARFIMA/FI-volatility (e.g., FIGARCH/FIEGARCH).\n",
    "\n",
    "Next step connection: Use these diagnostics to justify (i) ARIMAX order search with d=0 for returns, and (ii) FIGARCH/EGARCH (or FIEGARCH via R) for volatility with a fractional component if d suggests long memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d63e621-9054-408f-9781-6fc82704f26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Augmented Dickey–Fuller (ADF) on SPY_log_ret ===\n",
      "Test statistic: -17.7352 | p-value: 3.421e-30 | lags: 15 | nobs: 4879\n",
      "Critical values: 1%=-3.432, 5%=-2.862, 10%=-2.567\n",
      "Verdict (ADF): Reject unit root (stationary)\n",
      "\n",
      "=== Phillips–Perron (PP) on SPY_log_ret ===\n",
      "Test statistic: -78.0260 | p-value: 0 | lags: 32\n",
      "Critical values: {'1%': -3.432, '5%': -2.862, '10%': -2.567}\n",
      "Verdict (PP):  Reject unit root (stationary)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Stationarity Tests on SPY_log_ret\n",
    "# - Augmented Dickey–Fuller (ADF)\n",
    "# - Phillips–Perron (PP)\n",
    "# ===============================================================\n",
    "from arch.unitroot import PhillipsPerron\n",
    "\n",
    "# ---------- Helper Functions ----------\n",
    "def run_adf(x: pd.Series, autolag='AIC'):\n",
    "    x = pd.Series(x).dropna()\n",
    "    res = adfuller(x, autolag=autolag)\n",
    "    return {\n",
    "        'stat': res[0],\n",
    "        'pvalue': res[1],\n",
    "        'lags_used': res[2],\n",
    "        'nobs': res[3],\n",
    "        'crit': {'1%': res[4]['1%'], '5%': res[4]['5%'], '10%': res[4]['10%']},\n",
    "    }\n",
    "\n",
    "def run_pp(x: pd.Series, trend='c'):\n",
    "    \"\"\"\n",
    "    Phillips–Perron unit root test.\n",
    "    Compatible with arch versions that do not expose .bandwidth.\n",
    "    \"\"\"\n",
    "    x = pd.Series(x).dropna()\n",
    "    res = PhillipsPerron(x, trend=trend)\n",
    "    return {\n",
    "        'stat': res.stat,\n",
    "        'pvalue': res.pvalue,\n",
    "        'lags_used': getattr(res, 'lags', None),\n",
    "        'crit': getattr(res, 'critical_values', {}),\n",
    "        'summary': res.summary().as_text() if hasattr(res, 'summary') else None\n",
    "    }\n",
    "\n",
    "def stationarity_verdict(p):\n",
    "    return \"Reject unit root (stationary)\" if p < 0.05 else \"Fail to reject unit root (non-stationary)\"\n",
    "\n",
    "# ---------- Run Tests ----------\n",
    "y_ret = df['SPY_log_ret'].dropna()\n",
    "\n",
    "adf_res = run_adf(y_ret, autolag='AIC')\n",
    "pp_res  = run_pp(y_ret, trend='c')\n",
    "\n",
    "# ---------- Print Results ----------\n",
    "print(\"=== Augmented Dickey–Fuller (ADF) on SPY_log_ret ===\")\n",
    "print(f\"Test statistic: {adf_res['stat']:.4f} | p-value: {adf_res['pvalue']:.4g} \"\n",
    "      f\"| lags: {adf_res['lags_used']} | nobs: {adf_res['nobs']}\")\n",
    "print(f\"Critical values: 1%={adf_res['crit']['1%']:.3f}, 5%={adf_res['crit']['5%']:.3f}, 10%={adf_res['crit']['10%']:.3f}\")\n",
    "print(\"Verdict (ADF):\", stationarity_verdict(adf_res['pvalue']))\n",
    "\n",
    "print(\"\\n=== Phillips–Perron (PP) on SPY_log_ret ===\")\n",
    "print(f\"Test statistic: {pp_res['stat']:.4f} | p-value: {pp_res['pvalue']:.4g} | lags: {pp_res['lags_used']}\")\n",
    "print(f\"Critical values: { {k: round(v,3) for k,v in pp_res['crit'].items()} }\")\n",
    "print(\"Verdict (PP): \", stationarity_verdict(pp_res['pvalue']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f3f61-d7bd-4ab2-86e5-4b6752e2b767",
   "metadata": {},
   "source": [
    "# Long- Memory Test (R/S & GPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6a98014-0394-4b9b-b08d-3d7508f3ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Rescaled Range (R/S) — Hurst Exponent ===\n",
      "H (R/S): 0.542  ->  Long memory (persistent)\n",
      "\n",
      "=== GPH Log-Periodogram — Fractional d ===\n",
      "d_hat: -0.023  |  SE≈0.086  |  95% CI: (-0.19, 0.145)  |  m=69 of T=4895\n",
      "Interpretation (d): Anti-persistent / over-differenced\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Long-Memory Tests on SPY_log_ret\n",
    "# - Rescaled Range (R/S) -> Hurst exponent H\n",
    "# - Geweke–Porter–Hudak (GPH) -> fractional differencing d\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Helper Functions ----------\n",
    "def hurst_rs(x: pd.Series, min_window=16, max_window=None):\n",
    "    \"\"\"\n",
    "    Classic R/S estimator of Hurst exponent H.\n",
    "    H ~ 0.5 short memory; H>0.5 persistence; H<0.5 anti-persistence.\n",
    "    \"\"\"\n",
    "    x = pd.Series(x).dropna().values.astype(float)\n",
    "    n = x.size\n",
    "    if n < 64:\n",
    "        raise ValueError(\"Series too short for R/S estimation.\")\n",
    "    if max_window is None:\n",
    "        max_window = n // 4\n",
    "    wins = np.unique(np.floor(np.logspace(np.log10(min_window), np.log10(max_window), num=12)).astype(int))\n",
    "    RS = []\n",
    "    for w in wins:\n",
    "        if w < 8: \n",
    "            continue\n",
    "        blocks = n // w\n",
    "        if blocks < 2:\n",
    "            continue\n",
    "        vals = []\n",
    "        for b in range(blocks):\n",
    "            seg = x[b*w:(b+1)*w]\n",
    "            seg = seg - seg.mean()\n",
    "            y = np.cumsum(seg)\n",
    "            R = y.max() - y.min()\n",
    "            S = seg.std(ddof=1)\n",
    "            if S > 0:\n",
    "                vals.append(R / S)\n",
    "        if len(vals):\n",
    "            RS.append((w, np.mean(vals)))\n",
    "    RS = np.array(RS)\n",
    "    slope = np.polyfit(np.log(RS[:,0]), np.log(RS[:,1]), 1)[0]\n",
    "    return {'H': slope, 'windows': RS[:,0], 'RS': RS[:,1]}\n",
    "\n",
    "def gph_fractional_d(x: pd.Series, m=None):\n",
    "    \"\"\"\n",
    "    GPH (log-periodogram) estimator of fractional differencing parameter d.\n",
    "    Uses the regression: log I(λ_j) ~ a - d * log(4 sin^2(λ_j/2)) + u_j\n",
    "    => slope ≈ -d, so d_hat = -slope\n",
    "    \"\"\"\n",
    "    x = pd.Series(x).dropna().values.astype(float)\n",
    "    x = x - x.mean()\n",
    "    T = len(x)\n",
    "    if m is None:\n",
    "        m = int(T**0.5)  # common choice; try T^0.6 for robustness\n",
    "    j = np.arange(1, m+1)\n",
    "    lam = 2*np.pi*j/T\n",
    "    X = np.fft.fft(x)\n",
    "    I = (1/(2*np.pi*T)) * (np.abs(X[j])**2)\n",
    "    z = np.log(4*(np.sin(lam/2)**2))\n",
    "    y = np.log(I)\n",
    "    b = np.polyfit(z, y, 1)[0]  # slope\n",
    "    d_hat = -b\n",
    "    zc = z - z.mean()\n",
    "    se = np.pi / np.sqrt(6*np.sum(zc**2))  # asymptotic SE (approx)\n",
    "    ci = (d_hat - 1.96*se, d_hat + 1.96*se)\n",
    "    return {'d': d_hat, 'se': se, 'ci95': ci, 'm': m, 'T': T}\n",
    "\n",
    "def d_comment(d):\n",
    "    if d < 0:\n",
    "        return \"Anti-persistent / over-differenced\"\n",
    "    if 0 <= d < 0.5:\n",
    "        return \"Long memory but covariance-stationary (ARFIMA range)\"\n",
    "    if 0.5 <= d < 1:\n",
    "        return \"Non-stationary long memory (mean-reverting, infinite variance)\"\n",
    "    return \"Unit root or stronger non-stationarity\"\n",
    "\n",
    "# ---------- Run Tests ----------\n",
    "y_ret = df['SPY_log_ret'].dropna()\n",
    "\n",
    "rs_res  = hurst_rs(y_ret, min_window=16)\n",
    "gph_res = gph_fractional_d(y_ret, m=None)  # defaults to m = T^0.5\n",
    "\n",
    "# ---------- Print Results ----------\n",
    "print(\"=== Rescaled Range (R/S) — Hurst Exponent ===\")\n",
    "print(f\"H (R/S): {rs_res['H']:.3f}  ->  \"\n",
    "      f\"{'Long memory (persistent)' if rs_res['H']>0.5 else ('Anti-persistent' if rs_res['H']<0.5 else 'Short memory')}\")\n",
    "\n",
    "print(\"\\n=== GPH Log-Periodogram — Fractional d ===\")\n",
    "print(f\"d_hat: {gph_res['d']:.3f}  |  SE≈{gph_res['se']:.3f}  |  95% CI: \"\n",
    "      f\"{tuple(round(x,3) for x in gph_res['ci95'])}  |  m={gph_res['m']} of T={gph_res['T']}\")\n",
    "print(\"Interpretation (d):\", d_comment(gph_res['d']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218eb7a6-79b1-4add-a51a-7ded906a3c32",
   "metadata": {},
   "source": [
    "# Long-Memory Diagnostics on Volatility Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "446d1390-fbec-4967-b2f7-5ad64671ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute returns mean: 0.7876, variance: 9.0095e-01\n",
      "Squared returns mean: 1.5210e+00, variance: 3.7276e+01\n",
      "\n",
      "=== Long-Memory Diagnostics for |r_t| ===\n",
      "H (R/S): 0.885  ->  Long memory (persistent)\n",
      "d_hat: 0.435  |  SE≈0.086  |  95% CI: (0.267, 0.603)  |  m=69 of T=4895\n",
      "Interpretation: Long memory but covariance-stationary (FIEGARCH range)\n",
      "\n",
      "=== Long-Memory Diagnostics for r_t^2 ===\n",
      "H (R/S): 0.831  ->  Long memory (persistent)\n",
      "d_hat: 0.296  |  SE≈0.086  |  95% CI: (0.128, 0.464)  |  m=69 of T=4895\n",
      "Interpretation: Long memory but covariance-stationary (FIEGARCH range)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Long-Memory Diagnostics on Volatility Proxy\n",
    "# - Absolute and Squared Returns\n",
    "# ===============================================================\n",
    "# Use your return series\n",
    "y_ret = df['SPY_log_ret'].dropna()\n",
    "\n",
    "# Create volatility proxies\n",
    "abs_ret = np.abs(y_ret)\n",
    "sq_ret  = y_ret ** 2\n",
    "\n",
    "print(f\"Absolute returns mean: {abs_ret.mean():.4f}, variance: {abs_ret.var():.4e}\")\n",
    "print(f\"Squared returns mean: {sq_ret.mean():.4e}, variance: {sq_ret.var():.4e}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Reuse Hurst and GPH functions from previous cells\n",
    "# ---------------------------------------------------------------\n",
    "def hurst_rs(x: pd.Series, min_window=16, max_window=None):\n",
    "    x = pd.Series(x).dropna().values.astype(float)\n",
    "    n = x.size\n",
    "    if n < 64:\n",
    "        raise ValueError(\"Series too short for R/S estimation.\")\n",
    "    if max_window is None:\n",
    "        max_window = n // 4\n",
    "    wins = np.unique(np.floor(np.logspace(np.log10(min_window), np.log10(max_window), num=12)).astype(int))\n",
    "    RS = []\n",
    "    for w in wins:\n",
    "        if w < 8: \n",
    "            continue\n",
    "        blocks = n // w\n",
    "        if blocks < 2:\n",
    "            continue\n",
    "        vals = []\n",
    "        for b in range(blocks):\n",
    "            seg = x[b*w:(b+1)*w]\n",
    "            seg = seg - seg.mean()\n",
    "            y = np.cumsum(seg)\n",
    "            R = y.max() - y.min()\n",
    "            S = seg.std(ddof=1)\n",
    "            if S > 0:\n",
    "                vals.append(R / S)\n",
    "        if len(vals):\n",
    "            RS.append((w, np.mean(vals)))\n",
    "    RS = np.array(RS)\n",
    "    slope = np.polyfit(np.log(RS[:,0]), np.log(RS[:,1]), 1)[0]\n",
    "    return {'H': slope, 'windows': RS[:,0], 'RS': RS[:,1]}\n",
    "\n",
    "def gph_fractional_d(x: pd.Series, m=None):\n",
    "    x = pd.Series(x).dropna().values.astype(float)\n",
    "    x = x - x.mean()\n",
    "    T = len(x)\n",
    "    if m is None:\n",
    "        m = int(T**0.5)\n",
    "    j = np.arange(1, m+1)\n",
    "    lam = 2*np.pi*j/T\n",
    "    X = np.fft.fft(x)\n",
    "    I = (1/(2*np.pi*T)) * (np.abs(X[j])**2)\n",
    "    z = np.log(4*(np.sin(lam/2)**2))\n",
    "    y = np.log(I)\n",
    "    b = np.polyfit(z, y, 1)[0]\n",
    "    d_hat = -b\n",
    "    zc = z - z.mean()\n",
    "    se = np.pi / np.sqrt(6*np.sum(zc**2))\n",
    "    ci = (d_hat - 1.96*se, d_hat + 1.96*se)\n",
    "    return {'d': d_hat, 'se': se, 'ci95': ci, 'm': m, 'T': T}\n",
    "\n",
    "def d_comment(d):\n",
    "    if d < 0:\n",
    "        return \"Anti-persistent / over-differenced\"\n",
    "    if 0 <= d < 0.5:\n",
    "        return \"Long memory but covariance-stationary (FIEGARCH range)\"\n",
    "    if 0.5 <= d < 1:\n",
    "        return \"Non-stationary long memory (high persistence)\"\n",
    "    return \"Unit root or stronger non-stationarity\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Run diagnostics on |returns| and squared returns\n",
    "# ---------------------------------------------------------------\n",
    "for label, series in {\"|r_t|\": abs_ret, \"r_t^2\": sq_ret}.items():\n",
    "    print(f\"\\n=== Long-Memory Diagnostics for {label} ===\")\n",
    "    \n",
    "    rs_res = hurst_rs(series)\n",
    "    gph_res = gph_fractional_d(series)\n",
    "    \n",
    "    print(f\"H (R/S): {rs_res['H']:.3f}  ->  \"\n",
    "          f\"{'Long memory (persistent)' if rs_res['H']>0.5 else ('Anti-persistent' if rs_res['H']<0.5 else 'Short memory')}\")\n",
    "    \n",
    "    print(f\"d_hat: {gph_res['d']:.3f}  |  SE≈{gph_res['se']:.3f}  |  95% CI: \"\n",
    "          f\"{tuple(round(x,3) for x in gph_res['ci95'])}  |  m={gph_res['m']} of T={gph_res['T']}\")\n",
    "    print(\"Interpretation:\", d_comment(gph_res['d']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583256ac-77e0-4789-809f-313a958b9d5d",
   "metadata": {},
   "source": [
    "# Model Specification: Determine optimal orders for the ARIMAX (p,d,q) model and incorporate DI and WTI Oil returns as exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6155be4c-8a3e-485f-8f94-d520efe996b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ARIMAX order: (2, 0, 3) | AIC: 12806.21\n",
      "Residuals prepared for volatility model: (3916,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\anaconda3\\envs\\LSTM_Research_Project\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\josep\\anaconda3\\envs\\LSTM_Research_Project\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\josep\\anaconda3\\envs\\LSTM_Research_Project\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\josep\\anaconda3\\envs\\LSTM_Research_Project\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ARIMAX(p,d,q) with exogenous DXY_ret & WTI_ret\n",
    "# - auto_arima on train\n",
    "# - OOS preds for val/test\n",
    "# - residuals for volatility modeling\n",
    "# ===============================================================\n",
    "import pmdarima as pm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pmdarima\")\n",
    "\n",
    "TARGET = 'SPY_log_ret'\n",
    "EXOG   = ['DXY_ret', 'WTI_ret']\n",
    "\n",
    "def fit_arimax_auto(train_df, val_df=None, test_df=None, target=TARGET, exog_cols=EXOG):\n",
    "    y_tr   = train_df[target].dropna()\n",
    "    X_tr   = train_df[exog_cols].loc[y_tr.index]\n",
    "\n",
    "    model = pm.auto_arima(\n",
    "        y_tr, exogenous=X_tr, seasonal=False,\n",
    "        start_p=0, start_q=0, max_p=6, max_q=6, max_d=2,\n",
    "        information_criterion='aic', stepwise=True, trace=False,\n",
    "        error_action='ignore', suppress_warnings=True\n",
    "    )\n",
    "    print(\"Best ARIMAX order:\", model.order, \"| AIC:\", round(model.aic(), 2))\n",
    "\n",
    "    out = {'model': model}\n",
    "\n",
    "    # In-sample fitted + residuals (train)\n",
    "    y_hat_tr = pd.Series(model.predict_in_sample(exogenous=X_tr), index=y_tr.index, name='yhat_tr')\n",
    "    resid_tr = (y_tr - y_hat_tr).rename('resid_tr')\n",
    "    out.update({'yhat_tr': y_hat_tr, 'resid_tr': resid_tr})\n",
    "\n",
    "    # Validation OOS preds\n",
    "    if val_df is not None:\n",
    "        X_va = val_df[exog_cols]\n",
    "        y_hat_va = pd.Series(model.predict(n_periods=len(val_df), exogenous=X_va), index=val_df.index, name='yhat_va')\n",
    "        out['yhat_va'] = y_hat_va\n",
    "\n",
    "    # Test OOS preds\n",
    "    if test_df is not None:\n",
    "        X_te = test_df[exog_cols]\n",
    "        y_hat_te = pd.Series(model.predict(n_periods=len(test_df), exogenous=X_te), index=test_df.index, name='yhat_te')\n",
    "        out['yhat_te'] = y_hat_te\n",
    "\n",
    "    return out\n",
    "\n",
    "arimax_out = fit_arimax_auto(train_df, val_df, test_df)\n",
    "# Collect residuals for volatility model: use train (and optionally append val)\n",
    "resid_for_vol = arimax_out['resid_tr']  # + you can append (val - yhat_va) if you prefer\n",
    "print(\"Residuals prepared for volatility model:\", resid_for_vol.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065c060-a6c3-4787-afef-644f771b3c82",
   "metadata": {},
   "source": [
    "# Check ARIMAX residuals for ARCH effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "154d0f3b-c4aa-4f29-ab21-6c4cafc83b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ljung–Box p-values (resid):\n",
      "10    0.033076\n",
      "20    0.000027\n",
      "Name: lb_pvalue, dtype: float64\n",
      "\n",
      "ARCH LM p-value (lag 12): 1.448e-258  -> ARCH present\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_76596\\2476468035.py:11: FutureWarning: the 'maxlag' keyword is deprecated, use 'nlags' instead.\n",
      "  arch_stat, arch_pvalue, _, _ = het_arch(resid, maxlag=12)\n"
     ]
    }
   ],
   "source": [
    "# Check ARIMAX residuals for ARCH effects\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "resid = arimax_out['resid_tr'].dropna()\n",
    "\n",
    "# Ljung–Box on residuals (autocorrelation in mean)\n",
    "lb = acorr_ljungbox(resid, lags=[10,20], return_df=True)\n",
    "\n",
    "# Engle ARCH LM test (heteroskedasticity)\n",
    "arch_stat, arch_pvalue, _, _ = het_arch(resid, maxlag=12)\n",
    "\n",
    "print(\"Ljung–Box p-values (resid):\")\n",
    "print(lb['lb_pvalue'])\n",
    "print(f\"\\nARCH LM p-value (lag 12): {arch_pvalue:.4g}  -> \"\n",
    "      f\"{'ARCH present' if arch_pvalue < 0.05 else 'No strong ARCH'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94b18d-f583-4b7b-a940-a5690369c8b5",
   "metadata": {},
   "source": [
    "# Utilise Python proxy for model estimation: FIGARCH (long memory) or EGARCH (asymmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b6fb10a-4b7e-45d1-aa69-87a48c1710b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Zero Mean - FIGARCH Model Results                          \n",
      "====================================================================================\n",
      "Dep. Variable:                     resid_tr   R-squared:                       0.000\n",
      "Mean Model:                       Zero Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                          FIGARCH   Log-Likelihood:               -5074.10\n",
      "Distribution:      Standardized Student's t   AIC:                           10158.2\n",
      "Method:                  Maximum Likelihood   BIC:                           10189.6\n",
      "                                              No. Observations:                 3916\n",
      "Date:                      Fri, Nov 07 2025   Df Residuals:                     3916\n",
      "Time:                              14:58:08   Df Model:                            0\n",
      "                              Volatility Model                              \n",
      "============================================================================\n",
      "                 coef    std err          t      P>|t|      95.0% Conf. Int.\n",
      "----------------------------------------------------------------------------\n",
      "omega          0.0343  9.530e-03      3.602  3.158e-04 [1.565e-02,5.300e-02]\n",
      "phi        7.6238e-03  4.847e-02      0.157      0.875  [-8.738e-02,  0.103]\n",
      "d              0.6311  7.450e-02      8.471  2.423e-17     [  0.485,  0.777]\n",
      "beta           0.5587  8.522e-02      6.556  5.531e-11     [  0.392,  0.726]\n",
      "                              Distribution                              \n",
      "========================================================================\n",
      "                 coef    std err          t      P>|t|  95.0% Conf. Int.\n",
      "------------------------------------------------------------------------\n",
      "nu             5.3015      0.422     12.550  3.981e-36 [  4.474,  6.129]\n",
      "========================================================================\n",
      "\n",
      "Covariance estimator: robust\n",
      "Variance forecasts shape: (489,)\n"
     ]
    }
   ],
   "source": [
    "from arch.univariate import arch_model\n",
    "\n",
    "resid = arimax_out['resid_tr'].dropna()\n",
    "\n",
    "# Choose the proxy model\n",
    "VOL_KIND = 'FIGARCH'   # 'FIGARCH' (captures long memory d) or 'EGARCH' (captures asymmetry)\n",
    "DIST     = 't'\n",
    "\n",
    "if VOL_KIND.upper() == 'EGARCH':\n",
    "    am = arch_model(resid, mean='Zero', vol='EGARCH', p=1, o=1, q=1, dist=DIST)\n",
    "elif VOL_KIND.upper() == 'FIGARCH':\n",
    "    am = arch_model(resid, mean='Zero', vol='FIGARCH', p=1, q=1, dist=DIST)\n",
    "else:\n",
    "    raise ValueError(\"VOL_KIND must be 'EGARCH' or 'FIGARCH'\")\n",
    "\n",
    "vol_res = am.fit(disp='off')\n",
    "print(vol_res.summary())\n",
    "\n",
    "# Forecast variance for the validation window\n",
    "h_va = len(val_df)\n",
    "var_fc_va = vol_res.forecast(horizon=h_va, reindex=False).variance.values.ravel()\n",
    "var_fc_va = pd.Series(var_fc_va, index=val_df.index, name=f'var_fc_{VOL_KIND.lower()}')\n",
    "print(\"Variance forecasts shape:\", var_fc_va.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bccfed-97c8-41ca-91ac-2f39eb8c050d",
   "metadata": {},
   "source": [
    "# Compare EGARCH and FIGARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f22121eb-397d-484c-991c-e56758a974d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   LogLik         AIC         BIC  Leverage gamma  gamma p  \\\n",
      "Model                                                                        \n",
      "EGARCH(1,1)    -5003.1286  10016.2571  10047.6213         -0.1673      0.0   \n",
      "FIGARCH(1,d,1) -5074.0969  10158.1938  10189.5579             NaN      NaN   \n",
      "\n",
      "                d (FIGARCH)  d p  \n",
      "Model                             \n",
      "EGARCH(1,1)             NaN  NaN  \n",
      "FIGARCH(1,d,1)       0.6311  0.0  \n",
      "\n",
      "EGARCH standardized residual checks:\n",
      "  Ljung–Box p (10,20): 0.0004, 0.0005\n",
      "  ARCH LM p (lag 12): 0.8115\n",
      "\n",
      "FIGARCH standardized residual checks:\n",
      "  Ljung–Box p (10,20): 0.0000, 0.0001\n",
      "  ARCH LM p (lag 12): 0.06821\n",
      "\n",
      "Lower AIC -> EGARCH\n",
      "EGARCH leverage gamma=-0.1673, p=1.444e-30 -> material (leverage present) (negative -> leverage)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Compare EGARCH vs FIGARCH\n",
    "# ===========================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n",
    "\n",
    "resid = arimax_out['resid_tr'].dropna()\n",
    "\n",
    "def fit_egarch(resid, dist='t'):\n",
    "    m = arch_model(resid, mean='Zero', vol='EGARCH', p=1, o=1, q=1, dist=dist)\n",
    "    r = m.fit(disp='off')\n",
    "    # pull leverage coefficient (gamma[1]) and p-value\n",
    "    params = r.params\n",
    "    pvals  = r.pvalues\n",
    "    # arch names are typically: omega, alpha[1], gamma[1], beta[1], (nu)...\n",
    "    gamma = params.get('gamma[1]', np.nan)\n",
    "    gamma_p = pvals.get('gamma[1]', np.nan)\n",
    "    out = dict(model='EGARCH(1,1)', loglik=r.loglikelihood, aic=r.aic, bic=r.bic,\n",
    "               gamma=gamma, gamma_p=gamma_p, res=r)\n",
    "    return out\n",
    "\n",
    "def fit_figarch(resid, dist='t', p=1, q=1):\n",
    "    m = arch_model(resid, mean='Zero', vol='FIGARCH', p=p, q=q, dist=dist)\n",
    "    r = m.fit(disp='off')\n",
    "    d = r.params.get('d', np.nan)\n",
    "    d_p = r.pvalues.get('d', np.nan)\n",
    "    out = dict(model=f'FIGARCH({p},d,{q})', loglik=r.loglikelihood, aic=r.aic, bic=r.bic,\n",
    "               d=d, d_p=d_p, res=r)\n",
    "    return out\n",
    "\n",
    "eg = fit_egarch(resid, dist='t')\n",
    "fi = fit_figarch(resid, dist='t', p=1, q=1)   # you can also try q=0 if phi was insignificant\n",
    "\n",
    "# Summary table\n",
    "cmp = pd.DataFrame([\n",
    "    {'Model': eg['model'], 'LogLik': eg['loglik'], 'AIC': eg['aic'], 'BIC': eg['bic'],\n",
    "     'Leverage gamma': eg['gamma'], 'gamma p': eg['gamma_p'], 'd (FIGARCH)': np.nan, 'd p': np.nan},\n",
    "    {'Model': fi['model'], 'LogLik': fi['loglik'], 'AIC': fi['aic'], 'BIC': fi['bic'],\n",
    "     'Leverage gamma': np.nan, 'gamma p': np.nan, 'd (FIGARCH)': fi['d'], 'd p': fi['d_p']}\n",
    "]).set_index('Model')\n",
    "print(cmp.round(4))\n",
    "\n",
    "# Residual diagnostics for each model\n",
    "def diag(name, res_fit):\n",
    "    std = res_fit.std_resid.dropna()\n",
    "    lb = acorr_ljungbox(std, lags=[10,20], return_df=True)['lb_pvalue']\n",
    "    arch_p = het_arch(std, nlags=12)[1]  # use nlags (not maxlag)\n",
    "    print(f\"\\n{name} standardized residual checks:\")\n",
    "    print(f\"  Ljung–Box p (10,20): {lb.iloc[0]:.4f}, {lb.iloc[1]:.4f}\")\n",
    "    print(f\"  ARCH LM p (lag 12): {arch_p:.4g}\")\n",
    "\n",
    "diag('EGARCH', eg['res'])\n",
    "diag('FIGARCH', fi['res'])\n",
    "\n",
    "# Simple decision hints\n",
    "better = 'EGARCH' if eg['aic'] < fi['aic'] else 'FIGARCH'\n",
    "print(f\"\\nLower AIC -> {better}\")\n",
    "if not np.isnan(eg['gamma_p']):\n",
    "    print(f\"EGARCH leverage gamma={eg['gamma']:.4f}, p={eg['gamma_p']:.4g} \"\n",
    "          f\"-> {'material (leverage present)' if eg['gamma_p']<0.05 else 'not material'} \"\n",
    "          f\"{'(negative -> leverage)' if eg['gamma']<0 else '(positive)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab66ea-f781-4422-9f22-bb54acc34664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LSTM_Research_Project)",
   "language": "python",
   "name": "lstm_research_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
